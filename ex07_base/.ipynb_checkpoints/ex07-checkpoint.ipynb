{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center><i><b>\n",
    "Atenção: não são autorizadas cópias, divulgações ou qualquer tipo de uso deste material sem o consentimento prévio dos autores.\n",
    "</center></i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Exercício - Redes Neurais Artificiais </center>\n",
    "\n",
    "Neste exercício, você irá implementar uma rede neural artificial com *backpropagation* que será aplicada na tarefa de reconhecimento de dígitos manuscritos. Antes de iniciar, é fortemente recomendado que você revise o material apresentado em aula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O problema\n",
    "\n",
    "Você foi contratado por uma grande empresa para fazer a identificação correta e automática de quais dígitos estão presentes em um conjunto de imagens. Essas imagens têm dimensão de 20 x 20 pixels, onde cada pixel é representado por um ponto flutuante que indica a intensidade de tons de cinza naquela região.\n",
    "\n",
    "Sabe-se que a aplicação de redes neurais utilizando o algoritmo de *backpropagation* neste tipo de problema obtêm resultados satisfatórios. Assim, seu desafio é implementar tal algoritmo e encontrar os pesos ótimos para que a rede seja capaz de identificar automaticamente os dígitos contidos nas imagens.\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 0px; float: center;\">\n",
    "    <img src=\"figs/digitos.png\"  style=\"height:400px;\"/> \n",
    "    <center><em>Figura 1. Amostras do conjunto de dados.</em></center>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Carregando e visualizando os dados\n",
    "\n",
    "Nessa etapa, você irá completar a função para plotar os dados. O conjunto que você utilizará será de digitos manuscritos (Figura 1).\n",
    "\n",
    "Cada imagem tem dimensão de 20 x 20 pixels e cada pixel é representado por um ponto flutuante com a intensidade do tom de cinza naquela região. Deste modo, cada amostra é representada pelo desdobramento dos pixels em um vetor com 400 dimensões.\n",
    "\n",
    "O conjunto de dados contém 5.000 amostras, sendo cada amostra representada por um vetor com 400 dimensões. Portanto, o conjunto é representado por uma matriz [5000,400].\n",
    "\n",
    "A segunda parte do conjunto de dados é um vetor $y$ com 5.000 dimensões, o qual contém os rótulos para cada amostra da base de treino. Imagens contendo dígitos de 1 a 9 recebem, respectivamente, classes de 1 a 9, enquanto imagens contendo o dígito 0 são rotuladas como 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos carregar os dados do arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np  # importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "\n",
    "# importa o arquivo e guarda em um dataframe do Pandas\n",
    "df_dataset = pd.read_csv( 'dados.csv', sep=',', header=None) \n",
    "\n",
    "print('Dados carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos guardar os valores dentro de um array X e as classes dentro de um vetor Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: [10 10 10 10 10]\n",
      "\n",
      "Dimensao de X:  (5000, 400)\n",
      "\n",
      "Dimensao de Y:  (5000,)\n",
      "\n",
      "Classes do problema:  [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "# Pega os valores das n-1 primeiras colunas e guarda em uma matrix X\n",
    "X = df_dataset.iloc[:, 0:-1].values \n",
    "\n",
    "# Pega os valores da ultima coluna e guarda em um vetor Y\n",
    "Y = df_dataset.iloc[:, -1].values \n",
    "\n",
    "# Imprime as 5 primeiras linhas da matriz X\n",
    "display('X:', X[0:5,:])\n",
    "\n",
    "# Imprime os 5 primeiros valores de Y\n",
    "print('Y:', Y[0:5])\n",
    "\n",
    "print('\\nDimensao de X: ', X.shape)\n",
    "\n",
    "print('\\nDimensao de Y: ', Y.shape)\n",
    "\n",
    "print('\\nClasses do problema: ', np.unique(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar aleatoriamente 100 amostras da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9562d38fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizaDados(X):\n",
    "    example_width = int(round(np.sqrt(X.shape[1])) )\n",
    "\n",
    "    # Calcula numero de linhas e colunas\n",
    "    m, n = X.shape\n",
    "    example_height = int(n / example_width)\n",
    "\n",
    "    # Calcula numero de itens que serao exibidos\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, axs = plt.subplots(display_rows,display_cols, figsize=(7, 7))\n",
    "                        \n",
    "    for ax, i in zip(axs.ravel(), range( X.shape[0] )):\n",
    "    \n",
    "        new_X = np.reshape( np.ravel(X[i,:]), (example_width, example_height) )\n",
    "\n",
    "        ax.imshow(new_X.T, cmap='gray'); \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "idx_perm = np.random.permutation( range(X.shape[0]) )\n",
    "visualizaDados( X[idx_perm[0:100],:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Carregando os parâmetros\n",
    "\n",
    "A rede neural proposta para este exercício tem 3 camadas: uma camada de entrada, uma camada oculta e uma camada de saída  (Figura 2). É importante lembrar que a camada de entrada possui 400 neurônios devido ao desdobramento dos pixels das amostras em vetores com 400 dimensões (sem considerar o *bias*, sempre +1).\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 5px; float: center;\">\n",
    "    <img src=\"figs/nn.png\"  style=\"height:350px;\"/> \n",
    "    <center><em>Figura 2. Arquitetura da rede neural.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Inicialmente, você terá acesso aos parâmetros ($\\Theta^{(1)}$, $\\Theta^{(2)}$) de uma rede neural já treinada, armazenados nos arquivos **pesos_Theta1.csv** e **pesos_Theta2.csv**. Tais parâmetros têm dimensões condizentes com uma rede neural com 25 neurônios na camada intermediária e 10 neurônios na camada de saída (correspondente às dez possíveis classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos carregar os pesos pré-treinados para a rede neural e inicializar os parâmetros mais importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando parametros salvos da rede neural...\n",
      "\n",
      "Pesos carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# parametros a serem utilizados neste exercicio\n",
    "input_layer_size  = 400  # 20x20 dimensao das imagens de entrada\n",
    "hidden_layer_size = 25   # 25 neuronios na camada oculta\n",
    "num_labels = 10          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "    \n",
    "print('\\nCarregando parametros salvos da rede neural...\\n')\n",
    "\n",
    "# carregando os pesos da camada 1\n",
    "Theta1 = pd.read_csv('pesos_Theta1.csv', sep=',', header=None).values\n",
    "\n",
    "# carregando os pesos da camada 2\n",
    "Theta2 = pd.read_csv('pesos_Theta2.csv', sep=',', header=None).values\n",
    "\n",
    "# concatena os pesos em um único vetor\n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "\n",
    "print('Pesos carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Calcula o custo (*Feedforward*)\n",
    "\n",
    "Você precisará implementar a função de custo e gradiente para a rede neural. A função de custo (sem regularização) é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "Na função $J$, $h_\\Theta(x^{(i)})$ é computado conforme representado na Figura 2. A constante $K$ representa o número de classes. Assim, $h_\\Theta(x^{(i)})_k$ = $a^{(3)}_k$ corresponde à ativação (valor de saída) da $k$-ésima unidade de saída. Também, é importante lembrar que o vetor de saída precisa ser criado a partir da classe original, tornando-se compatível com a rede neural, ou seja, espera-se vetores com 10 posições contendo 1 para o elemento referente à classe esperada e 0 nos demais elementos. Por exemplo, seja 5 o rótulo de determinada amostra, o vetor $Y$ correspondente terá 1 na posição $y_5$ e 0 nas demais posições.\n",
    "\n",
    "Você precisará implementar o algoritmo *feedfoward* para calcular $h_\\Theta(x^{(i)})$ para cada amostra $i$ e somar o custo de todas as amostras. Seu código deverá ser flexível para funcionar com conjuntos de dados de qualquer tamanho e qualquer quantidade de classes, considerando $K \\geq 3$.\n",
    "\n",
    "Nesta fase, implemente a função de custo sem regularização para facilitar a análise. Posteriormente, você implementará o custo regularizado.\n",
    "\n",
    "Antes de implementar a função de custo, você precisará da função sigmoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid de 0 = 0.500000\n",
      "sigmoid de 10 = 0.999955\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula a função sigmoidal  \n",
    "    \"\"\"\n",
    "\n",
    "    z = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return z\n",
    "\n",
    "# testando a função sigmoidal\n",
    "z = sigmoid(0)\n",
    "print('sigmoid de 0 = %1.6f' %(z))\n",
    "\n",
    "z = sigmoid(10)\n",
    "print('sigmoid de 10 = %1.6f' %(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a função que será usada para calcular o custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Funcao de custo sem regularizacao ...\n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.287629 \n",
      "\n",
      "(este valor deve ser proximo de 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    #display(y[0:30])\n",
    "    \n",
    "    #a1 = (((-y) * np.log(z1).T) - ((1-y) * (np.log(1-z1).T) ))\n",
    "    #a1 = a1.T\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    #a3 = a3.T\n",
    "    #print(sum(sum(a2))/m)\n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "print('\\nFuncao de custo sem regularizacao ...\\n')\n",
    "\n",
    "J = funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f ' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.287629)\\n')\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Regularização\n",
    "\n",
    "A função de custo com regularização é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "$$ + \\frac{\\lambda}{2m} \\left[\\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left(\\Theta^{(1)}_{j,k}\\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left(\\Theta^{(2)}_{j,k}\\right)^2\\right]$$\n",
    "\n",
    "Você pode assumir que a rede neural terá 3 camadas - uma camada de entrada, uma camada oculta e uma camada de saída. No entanto, seu código deverá ser flexível para suportar qualquer quantidade de neurônios em cada uma dessas camadas. Embora a função $J$ descrita anteriormente contenha números fixos para $\\Theta^{(1)}$ e $\\Theta^{(2)}$, seu código deverá funcionar para outros tamanhos.\n",
    "\n",
    "Também, é importante que a regularização não seja aplicada a termos relacionados aos *bias*. Neste contexto, estes termos estão na primeira coluna de cada matriz $\\Theta^{(1)}$ e $\\Theta^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, complete a nova função de custo aplicando regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    #\n",
    "    # (3): Implemente a regularização na função de custo.\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    reg = sum(np.sum(np.power(Theta1[:,1:],2),axis=1)) + sum(np.sum(np.power(Theta2[:,1:],2),axis=1))\n",
    "    reg = (vLambda/(2 * m)) * (reg)      \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) + reg\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.383770 \n",
      "\n",
      "(este valor deve ser proximo de 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "# Parametro de regularizacao dos pesos (aqui sera igual a 1).\n",
    "vLambda = 1;\n",
    "\n",
    "J = funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f ' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.383770)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5: Inicialização dos parâmetros\n",
    "\n",
    "Nesta parte, começa a implementação das duas camadas da rede neural para classificação dos dígitos manuscritos. Os pesos são inicializados aleatoriamente. Mas, para que toda a execução gere o mesmo resultado, vamos usar uma semente para a função de geração de números aleatórios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicializando parametros da rede neural...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inicializaPesosAleatorios(L_in, L_out, randomSeed = None):\n",
    "    '''\n",
    "    Inicializa aleatoriamente os pesos de uma camada usando \n",
    "    L_in (conexoes de entrada) e L_out (conexoes de saida).\n",
    "\n",
    "    W sera definido como uma matriz de dimensoes [L_out, 1 + L_in]\n",
    "    visto que devera armazenar os termos para \"bias\".\n",
    "    \n",
    "    randomSeed: indica a semente para o gerador aleatorio\n",
    "    '''\n",
    "\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # se for fornecida uma semente para o gerador aleatorio\n",
    "    if randomSeed is not None:\n",
    "        W = np.random.RandomState(randomSeed).rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    # se nao for fornecida uma semente para o gerador aleatorio\n",
    "    else:\n",
    "        W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "print('\\nInicializando parametros da rede neural...\\n')\n",
    "    \n",
    "initial_Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "initial_Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)\n",
    "\n",
    "# junta os pesos iniciais em um unico vetor\n",
    "initial_rna_params = np.concatenate([np.ravel(initial_Theta1), np.ravel(initial_Theta2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Backpropagation\n",
    "\n",
    "Nesta parte do exercício, você implementará o algoritmo de *backpropagation* responsável por calcular o gradiente para a função de custo da rede neural. Terminada a implementação do cálculo do gradiente, você poderá treinar a rede neural minimizando a função de custo $J(\\Theta)$ usando um otimizador avançado, como a funcao `minimize` do ScyPy.\n",
    "\n",
    "Primeiro, você precisará implementar o gradiente para a rede neural sem regularização. Após ter verificado que o cálculo do gradiente está correto, você implementará o gradiente para a rede neural com regularização.\n",
    "\n",
    "Você deverá começar pela implementação do gradiente da sigmóide, o qual pode ser calculado utilizando a equação:\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)), $$\n",
    "\n",
    "sendo que\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}}. $$\n",
    "\n",
    "Ao completar, teste diferentes valores para a função `sigmoidGradient`. Para valores grandes de *z* (tanto positivo, quanto negativo), o resultado deve ser próximo a zero. Quando $z = 0$, o resultado deve ser exatamente 0,25. A função deve funcionar com vetores e matrizes também. No caso de matrizes, a função deve calcular o gradiente para cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando o gradiente da sigmoide...\n",
      "\n",
      "Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\n",
      "\n",
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n",
      "\n",
      "Se sua implementacao estiver correta, o gradiente da sigmoide sera:\n",
      "[0.19661193 0.23500371 0.25 0.23500371 0.19661193]:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    '''\n",
    "    Retorna o gradiente da funcao sigmoidal para z \n",
    "    \n",
    "    Calcula o gradiente da funcao sigmoidal\n",
    "    para z. A funcao deve funcionar independente se z for matriz ou vetor.\n",
    "    Nestes casos,  o gradiente deve ser calculado para cada elemento.\n",
    "    '''\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Calcula o gradiente da funcao sigmoidal para \n",
    "    #           cada valor de z (seja z matriz, escalar ou vetor).\n",
    "    #\n",
    "\n",
    "    g = (1/(1+np.exp(-z))) * (1- (1/(1+np.exp(-z))))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ##########################################################################\n",
    "\n",
    "    return g\n",
    "\n",
    "print('\\nAvaliando o gradiente da sigmoide...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([1,-0.5, 0, 0.5, 1]))\n",
    "print('Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\\n')\n",
    "print(g)\n",
    "\n",
    "print('\\nSe sua implementacao estiver correta, o gradiente da sigmoide sera:')\n",
    "print('[0.19661193 0.23500371 0.25 0.23500371 0.19661193]:\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do algoritmo *backpropagation* é encontrar a \"parcela de responsabilidade\" que cada neurônio da rede neural teve com o erro gerado na saída. Dada uma amostra de treino ($x^{(t)}, y^{(t)}$), primeiro é executado o passo de *feedforward* para calcular todas as ativações na rede, incluindo o valor de saída $h_{\\Theta}(x)$. Posteriormente, para cada neurônio $j$ na camada $l$, é calculado o \"erro\" $\\delta_{j}^{(l)}$ que mede o quanto determinado neurônio contribuiu para a diferença entre o valor esperado e o valor obtido na saída da rede.\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 5px; float: center;\">\n",
    "    <img src=\"figs/nn_back.png\"  style=\"height:350px;\"/> \n",
    "    <center><em>Figura 3. Arquitetura da rede neural.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Nos neurônios de saída, a diferença pode ser medida entre o valor esperado (rótulo da amostra) e o valor obtido (a ativação final da rede), onde tal diferença será usada para definir $\\delta_{j}^{(3)}$ (visto que a camada 3 é a última). Nas camadas ocultas (quando houver mais de uma), o termo $\\delta_{j}^{(l)}$ será calculado com base na média ponderada dos erros encontrados na camada posterior ($l + 1$).\n",
    "\n",
    "A seguir, é descrito em detalhes como a implementação do algoritmo *backpropagation* deve ser feita. Você precisará seguir os passos 1 a 4 dentro de um laço, processando uma amostra por vez. No passo 5, o gradiente acumulado é dividido pelas *m* amostras, o qual será utilizado na função de custo da rede neural.\n",
    "\n",
    " 1. Coloque os valores na camada de entrada ($a^{(1)}$) para a amostra de treino a ser processada. Calcule as ativações das camadas 2 e 3 utilizando o passo de *feedforward*. Observe que será necessário adicionar um termo $+1$ para garantir que os vetores de ativação das camadas ($a^{(1)}$) e ($a^{(2)}$) também incluam o neurônio de *bias*.\n",
    " \n",
    " 2. Para cada neurônio $k$ na camada 3 (camada de saída), defina:\n",
    "    $$ \\delta_{k}^{(3)} = (a^{(3)}_k - y_k), $$\n",
    "    onde $y_k \\in \\{0,1\\}$ indica se a amostra sendo processada pertence a classe $k$ ($y_k = 1$) ou não ($y_k = 0$).\n",
    "    \n",
    " 3. Para a camada oculta $l$ = 2, defina:\n",
    "\n",
    "    $$ \\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)}*g'(z^{(2)}) $$\n",
    "    \n",
    " 4. Acumule o gradiente usando a fórmula descrita a seguir. Lembre-se de não utilizar o valor associado ao bias $\\delta^{(2)}_0$.\n",
    "    \n",
    "    $$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T $$\n",
    "    \n",
    " 5. Obtenha o gradiente sem regularização para a função de custo da rede neural dividindo os gradientes acumulados por $\\frac{1}{m}$:\n",
    " \n",
    "     $$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} $$\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "Neste ponto, você deverá implementar o algoritmo de *backpropagation*. Crie uma nova função de custo, baseada na função implementada anteriormente, que retorne as derivadas parciais dos parâmetros. Nesta função, você precisará implementar o gradiente para a rede neural sem regularização.\n",
    "\n",
    "Logo após a função que implementa o algoritmo de *backpropagation*, é chamada uma outra função que fará a checagem do gradiente. O código dessa função está no arquivo **utils.py** que está localizado na pasta raíz desse exercício. Esta checagem tem o propósito de certificar que seu código calcula o gradiente corretamente. Neste passo, se sua implementação estiver correta, você deverá ver uma diferença **menor que 1e-9**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.23162247e-02   1.73828184e-04   2.61455144e-04   1.08701450e-04\n",
      "   3.92471369e-03   1.90101252e-04   2.22272331e-04   5.00872547e-05\n",
      "  -8.08459407e-03   3.13170587e-05  -2.17840341e-05  -5.48569864e-05\n",
      "  -1.26669105e-02  -1.56130210e-04  -2.45506163e-04  -1.09164881e-04\n",
      "  -5.59342547e-03  -2.00036572e-04  -2.43630220e-04  -6.32313673e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23162336e-02   1.73828568e-04   2.61455681e-04   1.08701646e-04\n",
      "   3.92469532e-03   1.90101024e-04   2.22271989e-04   5.00871122e-05\n",
      "  -8.08457051e-03   3.13173499e-05  -2.17835955e-05  -5.48568037e-05\n",
      "  -1.26668667e-02  -1.56129670e-04  -2.45505349e-04  -1.09164542e-04\n",
      "  -5.59340168e-03  -2.00036278e-04  -2.43629777e-04  -6.32311830e-05\n",
      "   3.09347155e-01   1.61059122e-01   1.47036251e-01   1.58268289e-01\n",
      "   1.57616417e-01   1.47236090e-01   1.08132386e-01   5.61603508e-02\n",
      "   5.19507602e-02   5.47350261e-02   5.53079604e-02   5.17749682e-02\n",
      "   1.06269754e-01   5.57581320e-02   5.05565174e-02   5.38801993e-02\n",
      "   5.47404057e-02   5.02926606e-02]\n",
      "[  1.23162157e-02   1.73827798e-04   2.61454605e-04   1.08701254e-04\n",
      "   3.92473205e-03   1.90101479e-04   2.22272673e-04   5.00873973e-05\n",
      "  -8.08461763e-03   3.13167674e-05  -2.17844728e-05  -5.48571692e-05\n",
      "  -1.26669544e-02  -1.56130751e-04  -2.45506977e-04  -1.09165221e-04\n",
      "  -5.59344925e-03  -2.00036865e-04  -2.43630662e-04  -6.32315516e-05\n",
      "   3.09348289e-01   1.61075154e-01   1.47036792e-01   1.58268866e-01\n",
      "   1.57616997e-01   1.47236630e-01   1.08133620e-01   5.61663927e-02\n",
      "   5.19513482e-02   5.47356550e-02   5.53085911e-02   5.17755556e-02\n",
      "   1.06270990e-01   5.57640769e-02   5.05571062e-02   5.38808290e-02\n",
      "   5.47410373e-02   5.02932489e-02]\n",
      "[  1.23162250e-02   1.73828225e-04   2.61455201e-04   1.08701471e-04\n",
      "   3.92471346e-03   1.90101171e-04   2.22272217e-04   5.00872119e-05\n",
      "  -8.08459378e-03   3.13171616e-05  -2.17838880e-05  -5.48569315e-05\n",
      "  -1.26669100e-02  -1.56130019e-04  -2.45505891e-04  -1.09164779e-04\n",
      "  -5.59342517e-03  -2.00036468e-04  -2.43630072e-04  -6.32313118e-05\n",
      "   3.09347715e-01   1.61067305e-01   1.47036518e-01   1.58268574e-01\n",
      "   1.57616704e-01   1.47236357e-01   1.08132995e-01   5.61633594e-02\n",
      "   5.19510505e-02   5.47353368e-02   5.53082717e-02   5.17752583e-02\n",
      "   1.06270365e-01   5.57607763e-02   5.05568081e-02   5.38805104e-02\n",
      "   5.47407175e-02   5.02929511e-02]\n",
      "[  1.23162243e-02   1.73828143e-04   2.61455087e-04   1.08701429e-04\n",
      "   3.92471391e-03   1.90101332e-04   2.22272445e-04   5.00872976e-05\n",
      "  -8.08459437e-03   3.13169557e-05  -2.17841803e-05  -5.48570414e-05\n",
      "  -1.26669111e-02  -1.56130402e-04  -2.45506435e-04  -1.09164984e-04\n",
      "  -5.59342576e-03  -2.00036676e-04  -2.43630367e-04  -6.32314228e-05\n",
      "   3.09347729e-01   1.61066970e-01   1.47036525e-01   1.58268581e-01\n",
      "   1.57616711e-01   1.47236364e-01   1.08133010e-01   5.61633840e-02\n",
      "   5.19510579e-02   5.47353443e-02   5.53082797e-02   5.17752655e-02\n",
      "   1.06270380e-01   5.57614326e-02   5.05568155e-02   5.38805179e-02\n",
      "   5.47407255e-02   5.02929584e-02]\n",
      "[  1.23162252e-02   1.73828241e-04   2.61455228e-04   1.08701484e-04\n",
      "   3.92471335e-03   1.90101138e-04   2.22272159e-04   5.00871818e-05\n",
      "  -8.08459364e-03   3.13172048e-05  -2.17838124e-05  -5.48568929e-05\n",
      "  -1.26669097e-02  -1.56129938e-04  -2.45505750e-04  -1.09164707e-04\n",
      "  -5.59342502e-03  -2.00036424e-04  -2.43629996e-04  -6.32312729e-05\n",
      "   3.09347712e-01   1.61067389e-01   1.47036516e-01   1.58268572e-01\n",
      "   1.57616702e-01   1.47236355e-01   1.08132991e-01   5.61630844e-02\n",
      "   5.19510486e-02   5.47353349e-02   5.53082697e-02   5.17752564e-02\n",
      "   1.06270361e-01   5.57608792e-02   5.05568062e-02   5.38805085e-02\n",
      "   5.47407155e-02   5.02929493e-02]\n",
      "[  1.23162241e-02   1.73828127e-04   2.61455059e-04   1.08701416e-04\n",
      "   3.92471403e-03   1.90101365e-04   2.22272504e-04   5.00873276e-05\n",
      "  -8.08459451e-03   3.13169126e-05  -2.17842559e-05  -5.48570800e-05\n",
      "  -1.26669113e-02  -1.56130482e-04  -2.45506576e-04  -1.09165055e-04\n",
      "  -5.59342591e-03  -2.00036719e-04  -2.43630443e-04  -6.32314617e-05\n",
      "   3.09347733e-01   1.61066886e-01   1.47036527e-01   1.58268583e-01\n",
      "   1.57616713e-01   1.47236365e-01   1.08133014e-01   5.61636590e-02\n",
      "   5.19510598e-02   5.47353461e-02   5.53082817e-02   5.17752674e-02\n",
      "   1.06270384e-01   5.57613297e-02   5.05568174e-02   5.38805198e-02\n",
      "   5.47407275e-02   5.02929602e-02]\n",
      "[  1.23162249e-02   1.73828205e-04   2.61455178e-04   1.08701466e-04\n",
      "   3.92471354e-03   1.90101209e-04   2.22272258e-04   5.00872188e-05\n",
      "  -8.08459389e-03   3.13171136e-05  -2.17839406e-05  -5.48569403e-05\n",
      "  -1.26669102e-02  -1.56130108e-04  -2.45505989e-04  -1.09164796e-04\n",
      "  -5.59342528e-03  -2.00036516e-04  -2.43630125e-04  -6.32313207e-05\n",
      "   3.09347718e-01   1.61067242e-01   1.47036519e-01   1.58268575e-01\n",
      "   1.57616705e-01   1.47236358e-01   1.08132998e-01   5.61630735e-02\n",
      "   5.19510519e-02   5.47353382e-02   5.53082732e-02   5.17752596e-02\n",
      "   1.06270367e-01   5.57611891e-02   5.05568094e-02   5.38805118e-02\n",
      "   5.47407190e-02   5.02929524e-02]\n",
      "[  1.23162245e-02   1.73828163e-04   2.61455110e-04   1.08701434e-04\n",
      "   3.92471383e-03   1.90101294e-04   2.22272404e-04   5.00872907e-05\n",
      "  -8.08459426e-03   3.13170037e-05  -2.17841277e-05  -5.48570326e-05\n",
      "  -1.26669109e-02  -1.56130313e-04  -2.45506337e-04  -1.09164967e-04\n",
      "  -5.59342565e-03  -2.00036627e-04  -2.43630314e-04  -6.32314138e-05\n",
      "   3.09347727e-01   1.61067033e-01   1.47036524e-01   1.58268580e-01\n",
      "   1.57616709e-01   1.47236362e-01   1.08133007e-01   5.61636699e-02\n",
      "   5.19510565e-02   5.47353429e-02   5.53082782e-02   5.17752642e-02\n",
      "   1.06270377e-01   5.57610198e-02   5.05568141e-02   5.38805165e-02\n",
      "   5.47407240e-02   5.02929570e-02]\n",
      "[  1.23162063e-02   1.73827957e-04   2.61454802e-04   1.08701308e-04\n",
      "   3.92468606e-03   1.90100328e-04   2.22271244e-04   5.00870040e-05\n",
      "  -8.08458600e-03   3.13171588e-05  -2.17838834e-05  -5.48569236e-05\n",
      "  -1.26668923e-02  -1.56129984e-04  -2.45505823e-04  -1.09164740e-04\n",
      "  -5.59341382e-03  -2.00036427e-04  -2.43630002e-04  -6.32312768e-05\n",
      "   3.09347634e-01   1.61067092e-01   1.47028765e-01   1.58268533e-01\n",
      "   1.57616662e-01   1.47236318e-01   1.08132746e-01   5.61632378e-02\n",
      "   5.19482325e-02   5.47352096e-02   5.53081444e-02   5.17751396e-02\n",
      "   1.06269967e-01   5.57608931e-02   5.05539681e-02   5.38803075e-02\n",
      "   5.47405143e-02   5.02927617e-02]\n",
      "[  1.23162430e-02   1.73828411e-04   2.61455486e-04   1.08701593e-04\n",
      "   3.92474130e-03   1.90102174e-04   2.22273417e-04   5.00875052e-05\n",
      "  -8.08460215e-03   3.13169586e-05  -2.17841849e-05  -5.48570493e-05\n",
      "  -1.26669288e-02  -1.56130436e-04  -2.45506503e-04  -1.09165023e-04\n",
      "  -5.59343711e-03  -2.00036716e-04  -2.43630437e-04  -6.32314578e-05\n",
      "   3.09347810e-01   1.61067183e-01   1.47044278e-01   1.58268622e-01\n",
      "   1.57616752e-01   1.47236402e-01   1.08133260e-01   5.61635056e-02\n",
      "   5.19538759e-02   5.47354714e-02   5.53084070e-02   5.17753842e-02\n",
      "   1.06270778e-01   5.57613158e-02   5.05596555e-02   5.38807208e-02\n",
      "   5.47409287e-02   5.02931477e-02]\n",
      "[  1.23162244e-02   1.73828104e-04   2.61455030e-04   1.08701407e-04\n",
      "   3.92471276e-03   1.90101126e-04   2.22272154e-04   5.00871887e-05\n",
      "  -8.08459397e-03   3.13170940e-05  -2.17839841e-05  -5.48569676e-05\n",
      "  -1.26669103e-02  -1.56130131e-04  -2.45506049e-04  -1.09164839e-04\n",
      "  -5.59342532e-03  -2.00036521e-04  -2.43630147e-04  -6.32313401e-05\n",
      "   3.09347721e-01   1.61067137e-01   1.47036692e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08132999e-01   5.61633700e-02\n",
      "   5.19510443e-02   5.47353390e-02   5.53082740e-02   5.17752604e-02\n",
      "   1.06270367e-01   5.57611018e-02   5.05564853e-02   5.38805117e-02\n",
      "   5.47407189e-02   5.02929524e-02]\n",
      "[  1.23162249e-02   1.73828264e-04   2.61455258e-04   1.08701493e-04\n",
      "   3.92471461e-03   1.90101377e-04   2.22272509e-04   5.00873208e-05\n",
      "  -8.08459417e-03   3.13170234e-05  -2.17840842e-05  -5.48570053e-05\n",
      "  -1.26669108e-02  -1.56130290e-04  -2.45506276e-04  -1.09164924e-04\n",
      "  -5.59342561e-03  -2.00036622e-04  -2.43630292e-04  -6.32313944e-05\n",
      "   3.09347723e-01   1.61067138e-01   1.47036352e-01   1.58268578e-01\n",
      "   1.57616708e-01   1.47236361e-01   1.08133006e-01   5.61633734e-02\n",
      "   5.19510641e-02   5.47353421e-02   5.53082774e-02   5.17752634e-02\n",
      "   1.06270377e-01   5.57611071e-02   5.05571383e-02   5.38805166e-02\n",
      "   5.47407241e-02   5.02929571e-02]\n",
      "[  1.23162243e-02   1.73828070e-04   2.61454971e-04   1.08701377e-04\n",
      "   3.92471260e-03   1.90101074e-04   2.22272070e-04   5.00871495e-05\n",
      "  -8.08459392e-03   3.13171088e-05  -2.17839581e-05  -5.48569544e-05\n",
      "  -1.26669102e-02  -1.56130097e-04  -2.45505991e-04  -1.09164809e-04\n",
      "  -5.59342525e-03  -2.00036499e-04  -2.43630110e-04  -6.32313210e-05\n",
      "   3.09347721e-01   1.61067137e-01   1.47036778e-01   1.58268577e-01\n",
      "   1.57616706e-01   1.47236359e-01   1.08132998e-01   5.61633692e-02\n",
      "   5.19507706e-02   5.47353382e-02   5.53082732e-02   5.17752596e-02\n",
      "   1.06270365e-01   5.57611005e-02   5.05565888e-02   5.38805104e-02\n",
      "   5.47407175e-02   5.02929511e-02]\n",
      "[  1.23162250e-02   1.73828298e-04   2.61455317e-04   1.08701523e-04\n",
      "   3.92471477e-03   1.90101429e-04   2.22272593e-04   5.00873600e-05\n",
      "  -8.08459422e-03   3.13170086e-05  -2.17841101e-05  -5.48570185e-05\n",
      "  -1.26669109e-02  -1.56130324e-04  -2.45506335e-04  -1.09164954e-04\n",
      "  -5.59342568e-03  -2.00036644e-04  -2.43630329e-04  -6.32314135e-05\n",
      "   3.09347724e-01   1.61067138e-01   1.47036266e-01   1.58268578e-01\n",
      "   1.57616708e-01   1.47236361e-01   1.08133007e-01   5.61633742e-02\n",
      "   5.19513378e-02   5.47353429e-02   5.53082782e-02   5.17752642e-02\n",
      "   1.06270380e-01   5.57611084e-02   5.05570347e-02   5.38805179e-02\n",
      "   5.47407255e-02   5.02929583e-02]\n",
      "[  1.23162245e-02   1.73828141e-04   2.61455071e-04   1.08701414e-04\n",
      "   3.92471344e-03   1.90101186e-04   2.22272226e-04   5.00872071e-05\n",
      "  -8.08459401e-03   3.13170775e-05  -2.17840021e-05  -5.48569706e-05\n",
      "  -1.26669104e-02  -1.56130168e-04  -2.45506090e-04  -1.09164846e-04\n",
      "  -5.59342538e-03  -2.00036544e-04  -2.43630173e-04  -6.32313445e-05\n",
      "   3.09347722e-01   1.61067137e-01   1.47036628e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133001e-01   5.61633707e-02\n",
      "   5.19507576e-02   5.47353395e-02   5.53082747e-02   5.17752609e-02\n",
      "   1.06270369e-01   5.57611028e-02   5.05568973e-02   5.38805126e-02\n",
      "   5.47407198e-02   5.02929532e-02]\n",
      "[  1.23162248e-02   1.73828227e-04   2.61455217e-04   1.08701486e-04\n",
      "   3.92471394e-03   1.90101318e-04   2.22272437e-04   5.00873024e-05\n",
      "  -8.08459414e-03   3.13170399e-05  -2.17840662e-05  -5.48570022e-05\n",
      "  -1.26669107e-02  -1.56130253e-04  -2.45506235e-04  -1.09164917e-04\n",
      "  -5.59342556e-03  -2.00036599e-04  -2.43630266e-04  -6.32313901e-05\n",
      "   3.09347723e-01   1.61067138e-01   1.47036415e-01   1.58268578e-01\n",
      "   1.57616708e-01   1.47236361e-01   1.08133005e-01   5.61633728e-02\n",
      "   5.19513508e-02   5.47353415e-02   5.53082768e-02   5.17752628e-02\n",
      "   1.06270375e-01   5.57611061e-02   5.05567262e-02   5.38805157e-02\n",
      "   5.47407232e-02   5.02929562e-02]\n",
      "[  1.23162482e-02   1.73828475e-04   2.61455582e-04   1.08701633e-04\n",
      "   3.92472176e-03   1.90101352e-04   2.22272482e-04   5.00873176e-05\n",
      "  -8.08462539e-03   3.13173237e-05  -2.17837548e-05  -5.48569497e-05\n",
      "  -1.26669347e-02  -1.56130509e-04  -2.45506612e-04  -1.09165069e-04\n",
      "  -5.59343668e-03  -2.00036710e-04  -2.43630429e-04  -6.32314543e-05\n",
      "   3.09348195e-01   1.61067384e-01   1.47036747e-01   1.58261089e-01\n",
      "   1.57616949e-01   1.47236585e-01   1.08133342e-01   5.61635488e-02\n",
      "   5.19512161e-02   5.47328107e-02   5.53084494e-02   5.17754236e-02\n",
      "   1.06270552e-01   5.57611982e-02   5.05568975e-02   5.38779496e-02\n",
      "   5.47408134e-02   5.02930403e-02]\n",
      "[  1.23162011e-02   1.73827893e-04   2.61454705e-04   1.08701267e-04\n",
      "   3.92470561e-03   1.90101151e-04   2.22272181e-04   5.00871919e-05\n",
      "  -8.08456272e-03   3.13167935e-05  -2.17843133e-05  -5.48570229e-05\n",
      "  -1.26668864e-02  -1.56129912e-04  -2.45505714e-04  -1.09164694e-04\n",
      "  -5.59341426e-03  -2.00036433e-04  -2.43630011e-04  -6.32312802e-05\n",
      "   3.09347250e-01   1.61066891e-01   1.47036296e-01   1.58276066e-01\n",
      "   1.57616466e-01   1.47236135e-01   1.08132663e-01   5.61631946e-02\n",
      "   5.19508923e-02   5.47378703e-02   5.53081021e-02   5.17751002e-02\n",
      "   1.06270192e-01   5.57610107e-02   5.05567261e-02   5.38830787e-02\n",
      "   5.47406296e-02   5.02928691e-02]\n",
      "[  1.23162250e-02   1.73828287e-04   2.61455290e-04   1.08701505e-04\n",
      "   3.92471379e-03   1.90101287e-04   2.22272381e-04   5.00872736e-05\n",
      "  -8.08459381e-03   3.13169210e-05  -2.17842292e-05  -5.48570596e-05\n",
      "  -1.26669108e-02  -1.56130316e-04  -2.45506312e-04  -1.09164938e-04\n",
      "  -5.59342561e-03  -2.00036621e-04  -2.43630289e-04  -6.32313934e-05\n",
      "   3.09347728e-01   1.61067141e-01   1.47036524e-01   1.58268751e-01\n",
      "   1.57616710e-01   1.47236363e-01   1.08133007e-01   5.61633739e-02\n",
      "   5.19510563e-02   5.47353343e-02   5.53082779e-02   5.17752639e-02\n",
      "   1.06270374e-01   5.57611056e-02   5.05568129e-02   5.38801906e-02\n",
      "   5.47407227e-02   5.02929558e-02]\n",
      "[  1.23162244e-02   1.73828081e-04   2.61454998e-04   1.08701395e-04\n",
      "   3.92471359e-03   1.90101216e-04   2.22272281e-04   5.00872359e-05\n",
      "  -8.08459434e-03   3.13171964e-05  -2.17838390e-05  -5.48569133e-05\n",
      "  -1.26669102e-02  -1.56130105e-04  -2.45506013e-04  -1.09164825e-04\n",
      "  -5.59342533e-03  -2.00036523e-04  -2.43630150e-04  -6.32313411e-05\n",
      "   3.09347716e-01   1.61067135e-01   1.47036519e-01   1.58268404e-01\n",
      "   1.57616704e-01   1.47236357e-01   1.08132998e-01   5.61633695e-02\n",
      "   5.19510522e-02   5.47353468e-02   5.53082735e-02   5.17752599e-02\n",
      "   1.06270370e-01   5.57611033e-02   5.05568107e-02   5.38808377e-02\n",
      "   5.47407203e-02   5.02929537e-02]\n",
      "[  1.23162251e-02   1.73828330e-04   2.61455365e-04   1.08701544e-04\n",
      "   3.92471384e-03   1.90101302e-04   2.22272407e-04   5.00872868e-05\n",
      "  -8.08459379e-03   3.13168636e-05  -2.17843291e-05  -5.48571101e-05\n",
      "  -1.26669110e-02  -1.56130360e-04  -2.45506390e-04  -1.09164977e-04\n",
      "  -5.59342568e-03  -2.00036641e-04  -2.43630325e-04  -6.32314118e-05\n",
      "   3.09347731e-01   1.61067142e-01   1.47036526e-01   1.58268839e-01\n",
      "   1.57616712e-01   1.47236364e-01   1.08133009e-01   5.61633751e-02\n",
      "   5.19510573e-02   5.47350619e-02   5.53082790e-02   5.17752649e-02\n",
      "   1.06270376e-01   5.57611062e-02   5.05568134e-02   5.38802961e-02\n",
      "   5.47407233e-02   5.02929563e-02]\n",
      "[  1.23162242e-02   1.73828038e-04   2.61454922e-04   1.08701357e-04\n",
      "   3.92471354e-03   1.90101202e-04   2.22272255e-04   5.00872227e-05\n",
      "  -8.08459435e-03   3.13172538e-05  -2.17837391e-05  -5.48568628e-05\n",
      "  -1.26669101e-02  -1.56130061e-04  -2.45505936e-04  -1.09164786e-04\n",
      "  -5.59342526e-03  -2.00036502e-04  -2.43630114e-04  -6.32313228e-05\n",
      "   3.09347713e-01   1.61067133e-01   1.47036517e-01   1.58268316e-01\n",
      "   1.57616703e-01   1.47236356e-01   1.08132996e-01   5.61633684e-02\n",
      "   5.19510511e-02   5.47356191e-02   5.53082724e-02   5.17752589e-02\n",
      "   1.06270369e-01   5.57611027e-02   5.05568101e-02   5.38807322e-02\n",
      "   5.47407197e-02   5.02929531e-02]\n",
      "[  1.23162248e-02   1.73828239e-04   2.61455237e-04   1.08701496e-04\n",
      "   3.92471375e-03   1.90101270e-04   2.22272363e-04   5.00872705e-05\n",
      "  -8.08459404e-03   3.13169856e-05  -2.17841578e-05  -5.48570470e-05\n",
      "  -1.26669107e-02  -1.56130267e-04  -2.45506259e-04  -1.09164929e-04\n",
      "  -5.59342555e-03  -2.00036598e-04  -2.43630264e-04  -6.32313892e-05\n",
      "   3.09347726e-01   1.61067140e-01   1.47036523e-01   1.58268686e-01\n",
      "   1.57616709e-01   1.47236362e-01   1.08133005e-01   5.61633731e-02\n",
      "   5.19510555e-02   5.47350457e-02   5.53082771e-02   5.17752632e-02\n",
      "   1.06270374e-01   5.57611052e-02   5.05568125e-02   5.38806021e-02\n",
      "   5.47407222e-02   5.02929554e-02]\n",
      "[  1.23162245e-02   1.73828129e-04   2.61455050e-04   1.08701404e-04\n",
      "   3.92471362e-03   1.90101233e-04   2.22272299e-04   5.00872389e-05\n",
      "  -8.08459411e-03   3.13171318e-05  -2.17839105e-05  -5.48569259e-05\n",
      "  -1.26669103e-02  -1.56130154e-04  -2.45506067e-04  -1.09164834e-04\n",
      "  -5.59342538e-03  -2.00036545e-04  -2.43630175e-04  -6.32313453e-05\n",
      "   3.09347719e-01   1.61067136e-01   1.47036520e-01   1.58268469e-01\n",
      "   1.57616705e-01   1.47236358e-01   1.08133000e-01   5.61633703e-02\n",
      "   5.19510529e-02   5.47356353e-02   5.53082743e-02   5.17752606e-02\n",
      "   1.06270371e-01   5.57611037e-02   5.05568111e-02   5.38804262e-02\n",
      "   5.47407208e-02   5.02929541e-02]\n",
      "[  1.23162685e-02   1.73828724e-04   2.61455958e-04   1.08701789e-04\n",
      "   3.92473196e-03   1.90101477e-04   2.22272672e-04   5.00873965e-05\n",
      "  -8.08461821e-03   3.13167604e-05  -2.17844834e-05  -5.48571736e-05\n",
      "  -1.26669829e-02  -1.56131570e-04  -2.45508205e-04  -1.09165729e-04\n",
      "  -5.59344925e-03  -2.00036865e-04  -2.43630662e-04  -6.32315514e-05\n",
      "   3.09348321e-01   1.61067450e-01   1.47036807e-01   1.58268883e-01\n",
      "   1.57609283e-01   1.47236645e-01   1.08133627e-01   5.61636972e-02\n",
      "   5.19513518e-02   5.47356587e-02   5.53058930e-02   5.17755592e-02\n",
      "   1.06270973e-01   5.57614174e-02   5.05570979e-02   5.38808201e-02\n",
      "   5.47383739e-02   5.02932406e-02]\n",
      "[  1.23161808e-02   1.73827643e-04   2.61454330e-04   1.08701111e-04\n",
      "   3.92469542e-03   1.90101026e-04   2.22271991e-04   5.00871130e-05\n",
      "  -8.08456994e-03   3.13173570e-05  -2.17835849e-05  -5.48567993e-05\n",
      "  -1.26668381e-02  -1.56128850e-04  -2.45504119e-04  -1.09164033e-04\n",
      "  -5.59340169e-03  -2.00036278e-04  -2.43629778e-04  -6.32311831e-05\n",
      "   3.09347123e-01   1.61066825e-01   1.47036236e-01   1.58268272e-01\n",
      "   1.57624132e-01   1.47236075e-01   1.08132378e-01   5.61630462e-02\n",
      "   5.19507567e-02   5.47350223e-02   5.53106584e-02   5.17749646e-02\n",
      "   1.06269772e-01   5.57607915e-02   5.05565257e-02   5.38802082e-02\n",
      "   5.47430690e-02   5.02926689e-02]\n",
      "[  1.23162252e-02   1.73828375e-04   2.61455416e-04   1.08701552e-04\n",
      "   3.92471391e-03   1.90101331e-04   2.22272445e-04   5.00872973e-05\n",
      "  -8.08459437e-03   3.13169532e-05  -2.17841838e-05  -5.48570427e-05\n",
      "  -1.26669119e-02  -1.56130527e-04  -2.45506611e-04  -1.09165049e-04\n",
      "  -5.59342576e-03  -2.00036675e-04  -2.43630367e-04  -6.32314227e-05\n",
      "   3.09347730e-01   1.61067141e-01   1.47036525e-01   1.58268581e-01\n",
      "   1.57616882e-01   1.47236364e-01   1.08133010e-01   5.61633758e-02\n",
      "   5.19510580e-02   5.47353443e-02   5.53082715e-02   5.17752656e-02\n",
      "   1.06270380e-01   5.57611084e-02   5.05568154e-02   5.38805178e-02\n",
      "   5.47404009e-02   5.02929583e-02]\n",
      "[  1.23162241e-02   1.73827992e-04   2.61454872e-04   1.08701348e-04\n",
      "   3.92471346e-03   1.90101172e-04   2.22272218e-04   5.00872121e-05\n",
      "  -8.08459378e-03   3.13171642e-05  -2.17838845e-05  -5.48569302e-05\n",
      "  -1.26669092e-02  -1.56129894e-04  -2.45505714e-04  -1.09164714e-04\n",
      "  -5.59342517e-03  -2.00036468e-04  -2.43630072e-04  -6.32313118e-05\n",
      "   3.09347715e-01   1.61067134e-01   1.47036518e-01   1.58268574e-01\n",
      "   1.57616532e-01   1.47236357e-01   1.08132995e-01   5.61633677e-02\n",
      "   5.19510505e-02   5.47353367e-02   5.53082799e-02   5.17752582e-02\n",
      "   1.06270365e-01   5.57611005e-02   5.05568082e-02   5.38805105e-02\n",
      "   5.47410421e-02   5.02929512e-02]\n",
      "[  1.23162255e-02   1.73828456e-04   2.61455556e-04   1.08701624e-04\n",
      "   3.92471403e-03   1.90101365e-04   2.22272503e-04   5.00873273e-05\n",
      "  -8.08459452e-03   3.13169090e-05  -2.17842613e-05  -5.48570822e-05\n",
      "  -1.26669126e-02  -1.56130659e-04  -2.45506843e-04  -1.09165168e-04\n",
      "  -5.59342591e-03  -2.00036719e-04  -2.43630443e-04  -6.32314617e-05\n",
      "   3.09347733e-01   1.61067143e-01   1.47036527e-01   1.58268583e-01\n",
      "   1.57616971e-01   1.47236366e-01   1.08133014e-01   5.61633778e-02\n",
      "   5.19510599e-02   5.47353462e-02   5.53080002e-02   5.17752674e-02\n",
      "   1.06270383e-01   5.57611103e-02   5.05568172e-02   5.38805196e-02\n",
      "   5.47405078e-02   5.02929601e-02]\n",
      "[  1.23162238e-02   1.73827912e-04   2.61454731e-04   1.08701276e-04\n",
      "   3.92471335e-03   1.90101138e-04   2.22272159e-04   5.00871822e-05\n",
      "  -8.08459362e-03   3.13172084e-05  -2.17838070e-05  -5.48568906e-05\n",
      "  -1.26669085e-02  -1.56129762e-04  -2.45505482e-04  -1.09164595e-04\n",
      "  -5.59342502e-03  -2.00036424e-04  -2.43629996e-04  -6.32312729e-05\n",
      "   3.09347711e-01   1.61067132e-01   1.47036516e-01   1.58268572e-01\n",
      "   1.57616444e-01   1.47236355e-01   1.08132991e-01   5.61633656e-02\n",
      "   5.19510486e-02   5.47353348e-02   5.53085512e-02   5.17752564e-02\n",
      "   1.06270361e-01   5.57610986e-02   5.05568063e-02   5.38805087e-02\n",
      "   5.47409351e-02   5.02929494e-02]\n",
      "[  1.23162250e-02   1.73828286e-04   2.61455318e-04   1.08701536e-04\n",
      "   3.92471383e-03   1.90101294e-04   2.22272404e-04   5.00872905e-05\n",
      "  -8.08459426e-03   3.13170024e-05  -2.17841299e-05  -5.48570337e-05\n",
      "  -1.26669114e-02  -1.56130378e-04  -2.45506450e-04  -1.09165023e-04\n",
      "  -5.59342565e-03  -2.00036627e-04  -2.43630314e-04  -6.32314138e-05\n",
      "   3.09347727e-01   1.61067140e-01   1.47036524e-01   1.58268580e-01\n",
      "   1.57616817e-01   1.47236362e-01   1.08133007e-01   5.61633743e-02\n",
      "   5.19510566e-02   5.47353429e-02   5.53079822e-02   5.17752642e-02\n",
      "   1.06270377e-01   5.57611069e-02   5.05568140e-02   5.38805164e-02\n",
      "   5.47408112e-02   5.02929570e-02]\n",
      "[  1.23162243e-02   1.73828082e-04   2.61454970e-04   1.08701364e-04\n",
      "   3.92471355e-03   1.90101209e-04   2.22272259e-04   5.00872190e-05\n",
      "  -8.08459389e-03   3.13171150e-05  -2.17839383e-05  -5.48569392e-05\n",
      "  -1.26669097e-02  -1.56130042e-04  -2.45505876e-04  -1.09164739e-04\n",
      "  -5.59342528e-03  -2.00036516e-04  -2.43630125e-04  -6.32313207e-05\n",
      "   3.09347718e-01   1.61067135e-01   1.47036519e-01   1.58268575e-01\n",
      "   1.57616597e-01   1.47236358e-01   1.08132998e-01   5.61633692e-02\n",
      "   5.19510518e-02   5.47353381e-02   5.53085692e-02   5.17752596e-02\n",
      "   1.06270368e-01   5.57611020e-02   5.05568095e-02   5.38805119e-02\n",
      "   5.47406318e-02   5.02929525e-02]\n",
      "[  1.23162484e-02   1.73828477e-04   2.61455586e-04   1.08701635e-04\n",
      "   3.92472533e-03   1.90101396e-04   2.22272549e-04   5.00873452e-05\n",
      "  -8.08460528e-03   3.13169200e-05  -2.17842431e-05  -5.48570735e-05\n",
      "  -1.26669343e-02  -1.56130504e-04  -2.45506605e-04  -1.09165066e-04\n",
      "  -5.59341277e-03  -2.00035780e-04  -2.43629318e-04  -6.32311844e-05\n",
      "   3.09347896e-01   1.61067228e-01   1.47036605e-01   1.58268666e-01\n",
      "   1.57616796e-01   1.47228727e-01   1.08133337e-01   5.61635461e-02\n",
      "   5.19512136e-02   5.47355110e-02   5.53084467e-02   5.17727226e-02\n",
      "   1.06270840e-01   5.57613485e-02   5.05570349e-02   5.38807527e-02\n",
      "   5.47409608e-02   5.02905284e-02]\n",
      "[  1.23162009e-02   1.73827890e-04   2.61454702e-04   1.08701266e-04\n",
      "   3.92470204e-03   1.90101107e-04   2.22272114e-04   5.00871642e-05\n",
      "  -8.08458286e-03   3.13171974e-05  -2.17838252e-05  -5.48568994e-05\n",
      "  -1.26668867e-02  -1.56129917e-04  -2.45505721e-04  -1.09164697e-04\n",
      "  -5.59343814e-03  -2.00037362e-04  -2.43631120e-04  -6.32315498e-05\n",
      "   3.09347548e-01   1.61067047e-01   1.47036439e-01   1.58268489e-01\n",
      "   1.57616618e-01   1.47243993e-01   1.08132668e-01   5.61631974e-02\n",
      "   5.19508948e-02   5.47351701e-02   5.53081048e-02   5.17778012e-02\n",
      "   1.06269904e-01   5.57608604e-02   5.05565887e-02   5.38802756e-02\n",
      "   5.47404822e-02   5.02953811e-02]\n",
      "[  1.23162250e-02   1.73828288e-04   2.61455291e-04   1.08701506e-04\n",
      "   3.92471383e-03   1.90101302e-04   2.22272404e-04   5.00872819e-05\n",
      "  -8.08459421e-03   3.13170097e-05  -2.17841037e-05  -5.48570126e-05\n",
      "  -1.26669108e-02  -1.56130314e-04  -2.45506310e-04  -1.09164937e-04\n",
      "  -5.59342468e-03  -2.00036510e-04  -2.43630134e-04  -6.32313365e-05\n",
      "   3.09347724e-01   1.61067139e-01   1.47036523e-01   1.58268579e-01\n",
      "   1.57616708e-01   1.47236532e-01   1.08133007e-01   5.61633739e-02\n",
      "   5.19510562e-02   5.47353425e-02   5.53082779e-02   5.17752556e-02\n",
      "   1.06270378e-01   5.57611075e-02   5.05568146e-02   5.38805170e-02\n",
      "   5.47407245e-02   5.02926335e-02]\n",
      "[  1.23162244e-02   1.73828080e-04   2.61454996e-04   1.08701395e-04\n",
      "   3.92471354e-03   1.90101201e-04   2.22272259e-04   5.00872276e-05\n",
      "  -8.08459394e-03   3.13171077e-05  -2.17839646e-05  -5.48569603e-05\n",
      "  -1.26669102e-02  -1.56130106e-04  -2.45506015e-04  -1.09164826e-04\n",
      "  -5.59342626e-03  -2.00036634e-04  -2.43630305e-04  -6.32313980e-05\n",
      "   3.09347720e-01   1.61067136e-01   1.47036521e-01   1.58268576e-01\n",
      "   1.57616706e-01   1.47236188e-01   1.08132998e-01   5.61633695e-02\n",
      "   5.19510522e-02   5.47353385e-02   5.53082735e-02   5.17752682e-02\n",
      "   1.06270366e-01   5.57611014e-02   5.05568090e-02   5.38805113e-02\n",
      "   5.47407185e-02   5.02932759e-02]\n",
      "[  1.23162251e-02   1.73828331e-04   2.61455368e-04   1.08701545e-04\n",
      "   3.92471390e-03   1.90101324e-04   2.22272441e-04   5.00873010e-05\n",
      "  -8.08459428e-03   3.13169892e-05  -2.17841397e-05  -5.48570309e-05\n",
      "  -1.26669110e-02  -1.56130358e-04  -2.45506387e-04  -1.09164976e-04\n",
      "  -5.59342457e-03  -2.00036486e-04  -2.43630099e-04  -6.32313229e-05\n",
      "   3.09347725e-01   1.61067139e-01   1.47036523e-01   1.58268579e-01\n",
      "   1.57616709e-01   1.47236619e-01   1.08133009e-01   5.61633750e-02\n",
      "   5.19510572e-02   5.47353436e-02   5.53082790e-02   5.17749837e-02\n",
      "   1.06270381e-01   5.57611090e-02   5.05568160e-02   5.38805184e-02\n",
      "   5.47407261e-02   5.02927397e-02]\n",
      "[  1.23162242e-02   1.73828036e-04   2.61454920e-04   1.08701356e-04\n",
      "   3.92471347e-03   1.90101179e-04   2.22272222e-04   5.00872085e-05\n",
      "  -8.08459387e-03   3.13171282e-05  -2.17839286e-05  -5.48569419e-05\n",
      "  -1.26669101e-02  -1.56130063e-04  -2.45505939e-04  -1.09164787e-04\n",
      "  -5.59342637e-03  -2.00036657e-04  -2.43630340e-04  -6.32314117e-05\n",
      "   3.09347719e-01   1.61067136e-01   1.47036520e-01   1.58268576e-01\n",
      "   1.57616705e-01   1.47236102e-01   1.08132996e-01   5.61633684e-02\n",
      "   5.19510512e-02   5.47353375e-02   5.53082724e-02   5.17755401e-02\n",
      "   1.06270363e-01   5.57610999e-02   5.05568075e-02   5.38805099e-02\n",
      "   5.47407169e-02   5.02931697e-02]\n",
      "[  1.23162248e-02   1.73828239e-04   2.61455238e-04   1.08701497e-04\n",
      "   3.92471378e-03   1.90101279e-04   2.22272378e-04   5.00872775e-05\n",
      "  -8.08459416e-03   3.13170325e-05  -2.17840786e-05  -5.48570084e-05\n",
      "  -1.26669107e-02  -1.56130266e-04  -2.45506257e-04  -1.09164928e-04\n",
      "  -5.59342528e-03  -2.00036541e-04  -2.43630175e-04  -6.32313500e-05\n",
      "   3.09347724e-01   1.61067138e-01   1.47036522e-01   1.58268578e-01\n",
      "   1.57616708e-01   1.47236468e-01   1.08133005e-01   5.61633731e-02\n",
      "   5.19510555e-02   5.47353418e-02   5.53082771e-02   5.17749676e-02\n",
      "   1.06270376e-01   5.57611064e-02   5.05568135e-02   5.38805159e-02\n",
      "   5.47407234e-02   5.02930436e-02]\n",
      "[  1.23162245e-02   1.73828128e-04   2.61455049e-04   1.08701404e-04\n",
      "   3.92471360e-03   1.90101224e-04   2.22272285e-04   5.00872319e-05\n",
      "  -8.08459399e-03   3.13170848e-05  -2.17839896e-05  -5.48569645e-05\n",
      "  -1.26669103e-02  -1.56130155e-04  -2.45506068e-04  -1.09164835e-04\n",
      "  -5.59342565e-03  -2.00036602e-04  -2.43630264e-04  -6.32313845e-05\n",
      "   3.09347721e-01   1.61067137e-01   1.47036521e-01   1.58268577e-01\n",
      "   1.57616706e-01   1.47236253e-01   1.08133000e-01   5.61633704e-02\n",
      "   5.19510529e-02   5.47353392e-02   5.53082744e-02   5.17755562e-02\n",
      "   1.06270369e-01   5.57611025e-02   5.05568100e-02   5.38805124e-02\n",
      "   5.47407196e-02   5.02928659e-02]\n",
      "[  1.23156576e-02   1.73821184e-04   2.61444601e-04   1.08697058e-04\n",
      "   3.92462572e-03   1.90100163e-04   2.22270691e-04   5.00865713e-05\n",
      "  -8.08412144e-03   3.13229078e-05  -2.17752259e-05  -5.48533173e-05\n",
      "  -1.26663117e-02  -1.56122821e-04  -2.45495034e-04  -1.09160245e-04\n",
      "  -5.59325130e-03  -2.00034420e-04  -2.43626979e-04  -6.32300168e-05\n",
      "   3.09322731e-01   1.61054112e-01   1.47024614e-01   1.58255843e-01\n",
      "   1.57603935e-01   1.47224464e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23167917e-02   1.73835184e-04   2.61465686e-04   1.08705843e-04\n",
      "   3.92480166e-03   1.90102341e-04   2.22273972e-04   5.00879382e-05\n",
      "  -8.08506671e-03   3.13112096e-05  -2.17928424e-05  -5.48606556e-05\n",
      "  -1.26675093e-02  -1.56137600e-04  -2.45517291e-04  -1.09169517e-04\n",
      "  -5.59359964e-03  -2.00038723e-04  -2.43633461e-04  -6.32327177e-05\n",
      "   3.09372713e-01   1.61080164e-01   1.47048429e-01   1.58281312e-01\n",
      "   1.57629479e-01   1.47248256e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23082090e-02   1.73995402e-04   2.61706933e-04   1.08806316e-04\n",
      "   3.92466783e-03   1.90100677e-04   2.22271468e-04   5.00868960e-05\n",
      "  -8.08434772e-03   3.13201426e-05  -2.17793981e-05  -5.48550606e-05\n",
      "  -1.26665984e-02  -1.56126314e-04  -2.45500305e-04  -1.09162448e-04\n",
      "  -5.59333468e-03  -2.00035437e-04  -2.43628514e-04  -6.32306584e-05\n",
      "   3.09334696e-01   1.61060348e-01   1.47030315e-01   1.58261940e-01\n",
      "   1.57610050e-01   1.47230160e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23242410e-02   1.73660974e-04   2.61203367e-04   1.08596589e-04\n",
      "   3.92475954e-03   1.90101826e-04   2.22273195e-04   5.00876134e-05\n",
      "  -8.08484042e-03   3.13139748e-05  -2.17886702e-05  -5.48589123e-05\n",
      "  -1.26672226e-02  -1.56134106e-04  -2.45512020e-04  -1.09167315e-04\n",
      "  -5.59351625e-03  -2.00037706e-04  -2.43631925e-04  -6.32320761e-05\n",
      "   3.09360748e-01   1.61073927e-01   1.47042728e-01   1.58275215e-01\n",
      "   1.57623364e-01   1.47242561e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159545e-02   1.73824779e-04   2.61450007e-04   1.08699304e-04\n",
      "   3.91695737e-03   1.90271252e-04   2.22528303e-04   5.01938580e-05\n",
      "  -8.08436888e-03   3.13199040e-05  -2.17797422e-05  -5.48551939e-05\n",
      "  -1.26666252e-02  -1.56126616e-04  -2.45500740e-04  -1.09162616e-04\n",
      "  -5.59334248e-03  -2.00035525e-04  -2.43628640e-04  -6.32307075e-05\n",
      "   3.09335815e-01   1.61060931e-01   1.47030848e-01   1.58262510e-01\n",
      "   1.57610622e-01   1.47230692e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23164949e-02   1.73831589e-04   2.61460281e-04   1.08703596e-04\n",
      "   3.93247060e-03   1.89931258e-04   2.22016371e-04   4.99806562e-05\n",
      "  -8.08481926e-03   3.13142134e-05  -2.17883260e-05  -5.48587790e-05\n",
      "  -1.26671958e-02  -1.56133805e-04  -2.45511586e-04  -1.09167146e-04\n",
      "  -5.59350845e-03  -2.00037618e-04  -2.43631799e-04  -6.32320271e-05\n",
      "   3.09359629e-01   1.61073344e-01   1.47042195e-01   1.58274645e-01\n",
      "   1.57622793e-01   1.47242028e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159357e-02   1.73824751e-04   2.61449975e-04   1.08699298e-04\n",
      "   3.92466886e-03   1.90100717e-04   2.22271527e-04   5.00869198e-05\n",
      "  -8.09208204e-03   3.14907953e-05  -2.15224040e-05  -5.47480043e-05\n",
      "  -1.26666054e-02  -1.56126587e-04  -2.45500707e-04  -1.09162610e-04\n",
      "  -5.59333672e-03  -2.00035516e-04  -2.43628631e-04  -6.32307055e-05\n",
      "   3.09334988e-01   1.61060500e-01   1.47030454e-01   1.58262088e-01\n",
      "   1.57610199e-01   1.47230299e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23165136e-02   1.73831616e-04   2.61460312e-04   1.08703603e-04\n",
      "   3.92475851e-03   1.90101786e-04   2.22273136e-04   5.00875897e-05\n",
      "  -8.07710547e-03   3.11433296e-05  -2.20456529e-05  -5.49659638e-05\n",
      "  -1.26672157e-02  -1.56133834e-04  -2.45511618e-04  -1.09167153e-04\n",
      "  -5.59351422e-03  -2.00037627e-04  -2.43631809e-04  -6.32320290e-05\n",
      "   3.09360457e-01   1.61073775e-01   1.47042589e-01   1.58275067e-01\n",
      "   1.57623215e-01   1.47242422e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159348e-02   1.73824502e-04   2.61449604e-04   1.08699146e-04\n",
      "   3.92466873e-03   1.90100679e-04   2.22271470e-04   5.00868963e-05\n",
      "  -8.08435253e-03   3.13201355e-05  -2.17794061e-05  -5.48550622e-05\n",
      "  -1.26743348e-02  -1.55955174e-04  -2.45242593e-04  -1.09055103e-04\n",
      "  -5.59333646e-03  -2.00035440e-04  -2.43628517e-04  -6.32306590e-05\n",
      "   3.09334950e-01   1.61060480e-01   1.47030436e-01   1.58262070e-01\n",
      "   1.57610180e-01   1.47230281e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23165145e-02   1.73831866e-04   2.61460683e-04   1.08703754e-04\n",
      "   3.92475865e-03   1.90101824e-04   2.22273193e-04   5.00876131e-05\n",
      "  -8.08483562e-03   3.13139819e-05  -2.17886621e-05  -5.48589107e-05\n",
      "  -1.26594856e-02  -1.56305238e-04  -2.45769720e-04  -1.09274655e-04\n",
      "  -5.59351448e-03  -2.00037704e-04  -2.43631923e-04  -6.32320755e-05\n",
      "   3.09360494e-01   1.61073795e-01   1.47042607e-01   1.58275085e-01\n",
      "   1.57623235e-01   1.47242440e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159547e-02   1.73824855e-04   2.61450120e-04   1.08699350e-04\n",
      "   3.92467181e-03   1.90100734e-04   2.22271550e-04   5.00869281e-05\n",
      "  -8.08436910e-03   3.13198401e-05  -2.17798371e-05  -5.48552326e-05\n",
      "  -1.26666255e-02  -1.56126696e-04  -2.45500860e-04  -1.09162665e-04\n",
      "  -5.60105805e-03  -1.99864820e-04  -2.43371614e-04  -6.31236694e-05\n",
      "   3.09335826e-01   1.61060937e-01   1.47030854e-01   1.58262516e-01\n",
      "   1.57610628e-01   1.47230698e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23164946e-02   1.73831512e-04   2.61460167e-04   1.08703550e-04\n",
      "   3.92475556e-03   1.90101769e-04   2.22273113e-04   5.00875814e-05\n",
      "  -8.08481905e-03   3.13142773e-05  -2.17882312e-05  -5.48587403e-05\n",
      "  -1.26671956e-02  -1.56133724e-04  -2.45511465e-04  -1.09167097e-04\n",
      "  -5.58579229e-03  -2.00208316e-04  -2.43888814e-04  -6.33390605e-05\n",
      "   3.09359618e-01   1.61073338e-01   1.47042189e-01   1.58274639e-01\n",
      "   1.57622787e-01   1.47242023e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23156076e-02   1.73820567e-04   2.61443672e-04   1.08696670e-04\n",
      "   3.92445676e-03   1.90098071e-04   2.22267541e-04   5.00852587e-05\n",
      "  -8.08425429e-03   3.13212636e-05  -2.17777018e-05  -5.48543487e-05\n",
      "  -1.26662860e-02  -1.56122504e-04  -2.45494557e-04  -1.09160046e-04\n",
      "  -5.59309097e-03  -2.00032439e-04  -2.43623995e-04  -6.32287737e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08108009e-01   5.61503446e-02\n",
      "   5.19391460e-02   5.47226048e-02   5.52955027e-02   5.17633651e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23168417e-02   1.73835800e-04   2.61466615e-04   1.08706230e-04\n",
      "   3.92497062e-03   1.90104432e-04   2.22277122e-04   5.00892507e-05\n",
      "  -8.08493385e-03   3.13128538e-05  -2.17903664e-05  -5.48596242e-05\n",
      "  -1.26675350e-02  -1.56137917e-04  -2.45517769e-04  -1.09169716e-04\n",
      "  -5.59375996e-03  -2.00040704e-04  -2.43636444e-04  -6.32339608e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08157996e-01   5.61763989e-02\n",
      "   5.19629624e-02   5.47480762e-02   5.53210487e-02   5.17871587e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23132041e-02   1.73815900e-04   2.61167839e-04   1.08403272e-04\n",
      "   3.92457977e-03   1.90099575e-04   2.22269810e-04   5.00862071e-05\n",
      "  -8.08441697e-03   3.13192757e-05  -2.17807012e-05  -5.48556019e-05\n",
      "  -1.26665850e-02  -1.56126147e-04  -2.45500054e-04  -1.09162344e-04\n",
      "  -5.59325112e-03  -2.00034393e-04  -2.43626943e-04  -6.32300059e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08119975e-01   5.61565816e-02\n",
      "   5.19448473e-02   5.47287024e-02   5.53016180e-02   5.17690610e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23192459e-02   1.73840476e-04   2.61742460e-04   1.08999634e-04\n",
      "   3.92484761e-03   1.90102929e-04   2.22274853e-04   5.00883024e-05\n",
      "  -8.08477118e-03   3.13148417e-05  -2.17873670e-05  -5.48583710e-05\n",
      "  -1.26672360e-02  -1.56134274e-04  -2.45512272e-04  -1.09167419e-04\n",
      "  -5.59359981e-03  -2.00038751e-04  -2.43633496e-04  -6.32327286e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08146030e-01   5.61701618e-02\n",
      "   5.19572611e-02   5.47419786e-02   5.53149334e-02   5.17814628e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159307e-02   1.73824479e-04   2.61449554e-04   1.08699115e-04\n",
      "   3.92189226e-03   1.90091374e-04   2.21988734e-04   4.97906754e-05\n",
      "  -8.08443218e-03   3.13191042e-05  -2.17809487e-05  -5.48556978e-05\n",
      "  -1.26666130e-02  -1.56126461e-04  -2.45500507e-04  -1.09162519e-04\n",
      "  -5.59326609e-03  -2.00034561e-04  -2.43627187e-04  -6.32301001e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08121094e-01   5.61571648e-02\n",
      "   5.19453803e-02   5.47292727e-02   5.53021898e-02   5.17695936e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23165187e-02   1.73831889e-04   2.61460734e-04   1.08703786e-04\n",
      "   3.92753571e-03   1.90111137e-04   2.22555940e-04   5.03838388e-05\n",
      "  -8.08475596e-03   3.13150132e-05  -2.17871196e-05  -5.48582751e-05\n",
      "  -1.26672081e-02  -1.56133959e-04  -2.45511818e-04  -1.09167244e-04\n",
      "  -5.59358484e-03  -2.00038582e-04  -2.43633253e-04  -6.32326344e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08144911e-01   5.61695786e-02\n",
      "   5.19567281e-02   5.47414083e-02   5.53143616e-02   5.17809302e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159102e-02   1.73824449e-04   2.61449520e-04   1.08699108e-04\n",
      "   3.92458277e-03   1.90099692e-04   2.22269982e-04   5.00862766e-05\n",
      "  -8.08712353e-03   3.13108062e-05  -2.20626548e-05  -5.51518127e-05\n",
      "  -1.26665923e-02  -1.56126431e-04  -2.45500473e-04  -1.09162512e-04\n",
      "  -5.59325502e-03  -2.00034545e-04  -2.43627168e-04  -6.32300963e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08120267e-01   5.61567336e-02\n",
      "   5.19449864e-02   5.47288505e-02   5.53017673e-02   5.17691997e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23165391e-02   1.73831919e-04   2.61460768e-04   1.08703792e-04\n",
      "   3.92484461e-03   1.90102812e-04   2.22274680e-04   5.00882329e-05\n",
      "  -8.08206398e-03   3.13233188e-05  -2.15054020e-05  -5.45621554e-05\n",
      "  -1.26672288e-02  -1.56133990e-04  -2.45511853e-04  -1.09167251e-04\n",
      "  -5.59359591e-03  -2.00038598e-04  -2.43633271e-04  -6.32326382e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08145738e-01   5.61700098e-02\n",
      "   5.19571220e-02   5.47418305e-02   5.53147841e-02   5.17813241e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159093e-02   1.73824177e-04   2.61449116e-04   1.08698943e-04\n",
      "   3.92458238e-03   1.90099578e-04   2.22269814e-04   5.00862080e-05\n",
      "  -8.08442043e-03   3.13192706e-05  -2.17807070e-05  -5.48556031e-05\n",
      "  -1.26692929e-02  -1.56134413e-04  -2.45781658e-04  -1.09458380e-04\n",
      "  -5.59325452e-03  -2.00034398e-04  -2.43626949e-04  -6.32300071e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08120230e-01   5.61567141e-02\n",
      "   5.19449683e-02   5.47288321e-02   5.53017478e-02   5.17691820e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23165400e-02   1.73832191e-04   2.61461172e-04   1.08703957e-04\n",
      "   3.92484499e-03   1.90102925e-04   2.22274848e-04   5.00883015e-05\n",
      "  -8.08476772e-03   3.13148468e-05  -2.17873612e-05  -5.48583698e-05\n",
      "  -1.26645275e-02  -1.56125999e-04  -2.45230655e-04  -1.08871378e-04\n",
      "  -5.59359641e-03  -2.00038746e-04  -2.43633490e-04  -6.32327275e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08145776e-01   5.61700294e-02\n",
      "   5.19571401e-02   5.47418489e-02   5.53148036e-02   5.17813418e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23159309e-02   1.73824562e-04   2.61449678e-04   1.08699165e-04\n",
      "   3.92459139e-03   1.90099739e-04   2.22270049e-04   5.00863006e-05\n",
      "  -8.08443234e-03   3.13190583e-05  -2.17810169e-05  -5.48557256e-05\n",
      "  -1.26666133e-02  -1.56126546e-04  -2.45500633e-04  -1.09162570e-04\n",
      "  -5.59596450e-03  -2.00042896e-04  -2.43908424e-04  -6.35256720e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08121106e-01   5.61571708e-02\n",
      "   5.19453859e-02   5.47292783e-02   5.53021958e-02   5.17695990e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23165184e-02   1.73831806e-04   2.61460610e-04   1.08703735e-04\n",
      "   3.92483599e-03   1.90102764e-04   2.22274614e-04   5.00882088e-05\n",
      "  -8.08475581e-03   3.13150591e-05  -2.17870514e-05  -5.48582473e-05\n",
      "  -1.26672078e-02  -1.56133875e-04  -2.45511693e-04  -1.09167193e-04\n",
      "  -5.59088583e-03  -2.00030240e-04  -2.43352004e-04  -6.29370579e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08144899e-01   5.61695726e-02\n",
      "   5.19567225e-02   5.47414027e-02   5.53143556e-02   5.17809248e-02\n",
      "   1.06270372e-01   5.57611045e-02   5.05568118e-02   5.38805141e-02\n",
      "   5.47407215e-02   5.02929547e-02]\n",
      "[  1.23156067e-02   1.73820557e-04   2.61443656e-04   1.08696664e-04\n",
      "   3.92430823e-03   1.90096232e-04   2.22264771e-04   5.00841049e-05\n",
      "  -8.08441424e-03   3.13192843e-05  -2.17806826e-05  -5.48555903e-05\n",
      "  -1.26663101e-02  -1.56122800e-04  -2.45495003e-04  -1.09160232e-04\n",
      "  -5.59295726e-03  -2.00030787e-04  -2.43621507e-04  -6.32277369e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06245376e-01   5.57480759e-02   5.05449022e-02   5.38677770e-02\n",
      "   5.47279471e-02   5.02810566e-02]\n",
      "[  1.23168426e-02   1.73835811e-04   2.61466631e-04   1.08706237e-04\n",
      "   3.92511914e-03   1.90106271e-04   2.22279891e-04   5.00904046e-05\n",
      "  -8.08477391e-03   3.13148331e-05  -2.17873856e-05  -5.48583825e-05\n",
      "  -1.26675110e-02  -1.56137620e-04  -2.45517322e-04  -1.09169530e-04\n",
      "  -5.59389368e-03  -2.00042356e-04  -2.43638932e-04  -6.32349976e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06295368e-01   5.57741330e-02   5.05687213e-02   5.38932512e-02\n",
      "   5.47534959e-02   5.03048528e-02]\n",
      "[  1.23132525e-02   1.73500075e-04   2.61229877e-04   1.08786135e-04\n",
      "   3.92450235e-03   1.90098605e-04   2.22268352e-04   5.00856014e-05\n",
      "  -8.08450034e-03   3.13182321e-05  -2.17822701e-05  -5.48562537e-05\n",
      "  -1.26665976e-02  -1.56126303e-04  -2.45500289e-04  -1.09162441e-04\n",
      "  -5.59318142e-03  -2.00033522e-04  -2.43625634e-04  -6.32294617e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06257344e-01   5.57543136e-02   5.05506042e-02   5.38738754e-02\n",
      "   5.47340631e-02   5.02867532e-02]\n",
      "[  1.23191974e-02   1.74156301e-04   2.61680423e-04   1.08616771e-04\n",
      "   3.92492502e-03   1.90103898e-04   2.22276310e-04   5.00889080e-05\n",
      "  -8.08468781e-03   3.13158853e-05  -2.17857981e-05  -5.48577192e-05\n",
      "  -1.26672235e-02  -1.56134117e-04  -2.45512036e-04  -1.09167322e-04\n",
      "  -5.59366951e-03  -2.00039622e-04  -2.43634805e-04  -6.32332728e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06283401e-01   5.57678953e-02   5.05630193e-02   5.38871529e-02\n",
      "   5.47473799e-02   5.02991563e-02]\n",
      "[  1.23159303e-02   1.73824473e-04   2.61449546e-04   1.08699112e-04\n",
      "   3.92187031e-03   1.89774755e-04   2.22049380e-04   5.01728290e-05\n",
      "  -8.08450839e-03   3.13181413e-05  -2.17824011e-05  -5.48563044e-05\n",
      "  -1.26666244e-02  -1.56126606e-04  -2.45500725e-04  -1.09162610e-04\n",
      "  -5.59320238e-03  -2.00033758e-04  -2.43625974e-04  -6.32295936e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06258463e-01   5.57548969e-02   5.05511373e-02   5.38744457e-02\n",
      "   5.47346349e-02   5.02872858e-02]\n",
      "[  1.23165191e-02   1.73831894e-04   2.61460741e-04   1.08703789e-04\n",
      "   3.92755765e-03   1.90427756e-04   2.22495294e-04   5.00016852e-05\n",
      "  -8.08467976e-03   3.13159761e-05  -2.17856672e-05  -5.48576685e-05\n",
      "  -1.26671966e-02  -1.56133815e-04  -2.45511601e-04  -1.09167153e-04\n",
      "  -5.59364855e-03  -2.00039386e-04  -2.43634465e-04  -6.32331410e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06282282e-01   5.57673120e-02   5.05624863e-02   5.38865826e-02\n",
      "   5.47468081e-02   5.02986237e-02]\n",
      "[  1.23159098e-02   1.73824444e-04   2.61449512e-04   1.08699105e-04\n",
      "   3.92450708e-03   1.90098790e-04   2.22268625e-04   5.00857111e-05\n",
      "  -8.08715832e-03   3.09935038e-05  -2.20020632e-05  -5.47690348e-05\n",
      "  -1.26666046e-02  -1.56126577e-04  -2.45500692e-04  -1.09162603e-04\n",
      "  -5.59318688e-03  -2.00033735e-04  -2.43625948e-04  -6.32295883e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06257635e-01   5.57544657e-02   5.05507433e-02   5.38740234e-02\n",
      "   5.47342124e-02   5.02868919e-02]\n",
      "[  1.23165395e-02   1.73831924e-04   2.61460775e-04   1.08703796e-04\n",
      "   3.92492029e-03   1.90103713e-04   2.22276038e-04   5.00887984e-05\n",
      "  -8.08202919e-03   3.16406211e-05  -2.15659937e-05  -5.49449333e-05\n",
      "  -1.26672165e-02  -1.56133844e-04  -2.45511634e-04  -1.09167159e-04\n",
      "  -5.59366405e-03  -2.00039408e-04  -2.43634491e-04  -6.32331463e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06283109e-01   5.57677432e-02   5.05628802e-02   5.38870049e-02\n",
      "   5.47472306e-02   5.02990176e-02]\n",
      "[  1.23159089e-02   1.73824171e-04   2.61449108e-04   1.08698940e-04\n",
      "   3.92450648e-03   1.90098611e-04   2.22268359e-04   5.00856029e-05\n",
      "  -8.08450217e-03   3.13182294e-05  -2.17822732e-05  -5.48562543e-05\n",
      "  -1.26692578e-02  -1.56450819e-04  -2.45719806e-04  -1.09075137e-04\n",
      "  -5.59318618e-03  -2.00033529e-04  -2.43625642e-04  -6.32294633e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06257598e-01   5.57544461e-02   5.05507252e-02   5.38740051e-02\n",
      "   5.47341929e-02   5.02868742e-02]\n",
      "[  1.23165404e-02   1.73832196e-04   2.61461180e-04   1.08703960e-04\n",
      "   3.92492090e-03   1.90103892e-04   2.22276303e-04   5.00889066e-05\n",
      "  -8.08468598e-03   3.13158880e-05  -2.17857951e-05  -5.48577186e-05\n",
      "  -1.26645626e-02  -1.55809594e-04  -2.45292507e-04  -1.09254621e-04\n",
      "  -5.59366475e-03  -2.00039615e-04  -2.43634798e-04  -6.32332712e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06283147e-01   5.57677628e-02   5.05628983e-02   5.38870232e-02\n",
      "   5.47472501e-02   5.02990353e-02]\n",
      "[  1.23159305e-02   1.73824557e-04   2.61449670e-04   1.08699162e-04\n",
      "   3.92452069e-03   1.90098865e-04   2.22268729e-04   5.00857491e-05\n",
      "  -8.08450847e-03   3.13181170e-05  -2.17824372e-05  -5.48563191e-05\n",
      "  -1.26666247e-02  -1.56126687e-04  -2.45500845e-04  -1.09162659e-04\n",
      "  -5.59585154e-03  -2.00357777e-04  -2.43845219e-04  -6.31424915e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06258474e-01   5.57549029e-02   5.05511428e-02   5.38744513e-02\n",
      "   5.47346409e-02   5.02872913e-02]\n",
      "[  1.23165188e-02   1.73831811e-04   2.61460617e-04   1.08703738e-04\n",
      "   3.92490668e-03   1.90103639e-04   2.22275934e-04   5.00887604e-05\n",
      "  -8.08467968e-03   3.13160004e-05  -2.17856311e-05  -5.48576538e-05\n",
      "  -1.26671963e-02  -1.56133734e-04  -2.45511480e-04  -1.09167104e-04\n",
      "  -5.59099880e-03  -1.99715359e-04  -2.43415209e-04  -6.33202384e-05\n",
      "   3.09347722e-01   1.61067138e-01   1.47036522e-01   1.58268577e-01\n",
      "   1.57616707e-01   1.47236360e-01   1.08133003e-01   5.61633717e-02\n",
      "   5.19510542e-02   5.47353405e-02   5.53082757e-02   5.17752619e-02\n",
      "   1.06282270e-01   5.57673060e-02   5.05624807e-02   5.38865770e-02\n",
      "   5.47468021e-02   5.02986182e-02]\n",
      "[[  1.23162247e-02   1.23162247e-02]\n",
      " [  1.73828181e-04   1.73828184e-04]\n",
      " [  2.61455144e-04   2.61455144e-04]\n",
      " [  1.08701452e-04   1.08701450e-04]\n",
      " [  3.92471369e-03   3.92471369e-03]\n",
      " [  1.90101250e-04   1.90101252e-04]\n",
      " [  2.22272332e-04   2.22272331e-04]\n",
      " [  5.00872566e-05   5.00872547e-05]\n",
      " [ -8.08459407e-03  -8.08459407e-03]\n",
      " [  3.13170578e-05   3.13170587e-05]\n",
      " [ -2.17840324e-05  -2.17840341e-05]\n",
      " [ -5.48569878e-05  -5.48569864e-05]\n",
      " [ -1.26669105e-02  -1.26669105e-02]\n",
      " [ -1.56130211e-04  -1.56130210e-04]\n",
      " [ -2.45506162e-04  -2.45506163e-04]\n",
      " [ -1.09164882e-04  -1.09164881e-04]\n",
      " [ -5.59342547e-03  -5.59342547e-03]\n",
      " [ -2.00036570e-04  -2.00036572e-04]\n",
      " [ -2.43630220e-04  -2.43630220e-04]\n",
      " [ -6.32313690e-05  -6.32313673e-05]\n",
      " [  3.09347722e-01   3.09347722e-01]\n",
      " [  1.61067138e-01   1.61067138e-01]\n",
      " [  1.47036522e-01   1.47036522e-01]\n",
      " [  1.58268577e-01   1.58268577e-01]\n",
      " [  1.57616707e-01   1.57616707e-01]\n",
      " [  1.47236360e-01   1.47236360e-01]\n",
      " [  1.08133003e-01   1.08133003e-01]\n",
      " [  5.61633717e-02   5.61633717e-02]\n",
      " [  5.19510542e-02   5.19510542e-02]\n",
      " [  5.47353405e-02   5.47353405e-02]\n",
      " [  5.53082757e-02   5.53082757e-02]\n",
      " [  5.17752619e-02   5.17752619e-02]\n",
      " [  1.06270372e-01   1.06270372e-01]\n",
      " [  5.57611045e-02   5.57611045e-02]\n",
      " [  5.05568118e-02   5.05568118e-02]\n",
      " [  5.38805141e-02   5.38805141e-02]\n",
      " [  5.47407215e-02   5.47407215e-02]\n",
      " [  5.02929547e-02   5.02929547e-02]]\n",
      "As duas colunas acima deve ser bem semelhantes.\n",
      "(Esquerda - Gradiente numerico, Direita - Seu gradiente)\n",
      "\n",
      "Se sua implementacao de backpropagation esta correta, \n",
      "a diferenca relativa devera ser pequena (menor que 1e-9). \n",
      "\n",
      "Diferenca relativa: 2.16056e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para a tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i] :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "\n",
    "    g2 = np.zeros(Theta2[:,1:].shape)\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1))\n",
    "    \n",
    "    g3 = (a3 - y_rotulo)\n",
    "        \n",
    "    g2 = ((g3).dot(Theta2[:,1:]) * (sigmoidGradient(z2)))\n",
    "\n",
    "    a2i = np.insert(a2,0,1, axis = 1)\n",
    "        \n",
    "    #print(g3.shape)\n",
    "    #print(a2i.T.shape)\n",
    "    #print(Theta2_grad.shape)\n",
    "    Theta2_grad = Theta2_grad + (g3.T.dot(a2i))   \n",
    "        \n",
    "        #print(X[i].reshape(len(X[i]),1).shape)\n",
    "    xi = np.insert(X,0,1,axis = 1)\n",
    "    Theta1_grad = Theta1_grad + (g2.T.dot(xi))  \n",
    "        #print('-----------------------------')\n",
    "        #print(Theta1_grad/m)\n",
    "    Theta2_grad = Theta2_grad/m\n",
    "    Theta1_grad = Theta1_grad/m\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "    print(grad)\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# executa o arquivo que contem a funca que faz a checagem do gradiente\n",
    "%run utils.py\n",
    "verificaGradiente(funcaoCusto_backp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Outra parte da regularização\n",
    "\n",
    "Agora, a regularização deverá ser adicionada após se calcular o gradiente durante o algoritmo de *backpropagation*. Lembre-se que a regularização não é adicionada quando $j = 0$, ou seja, na primeira coluna de $\\Theta$. Portanto, para $j \\geq 1$, o gradiente é descrito como:\n",
    "\n",
    "$$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} + \\frac{\\lambda}{m}\\Theta^{(l)}_{ij} $$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Você deverá criar uma nova função de custo que é uma atualização da função anterior, mas com regularização e gradiente.\n",
    "\n",
    "Logo após a função que implementa o algoritmo de *backpropagation* com regularização, a função que faz a checagem do gradiente será chamada novamente. Se sua implementação estiver correta, você deverá ver uma diferença **menor que 1e-9**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1433733821\n",
      "2.14337215048\n",
      "2.14337461373\n",
      "2.14336791194\n",
      "2.14337885827\n",
      "2.14337251224\n",
      "2.14337425797\n",
      "2.14337791505\n",
      "2.14336885516\n",
      "2.14337298963\n",
      "2.14337377458\n",
      "2.14337504259\n",
      "2.14337172762\n",
      "2.14336942096\n",
      "2.14337734925\n",
      "2.14336744395\n",
      "2.14337932626\n",
      "2.14337419057\n",
      "2.14337257365\n",
      "2.1433766461\n",
      "2.14337012411\n",
      "2.14337938722\n",
      "2.14336738299\n",
      "2.14337661003\n",
      "2.14337016018\n",
      "2.1433746488\n",
      "2.14337211542\n",
      "2.14336745707\n",
      "2.14337931314\n",
      "2.14336950793\n",
      "2.14337726228\n",
      "2.14337512344\n",
      "2.14337164677\n",
      "2.14337394145\n",
      "2.14337282276\n",
      "2.14337791103\n",
      "2.14336885918\n",
      "2.1433725102\n",
      "2.14337426001\n",
      "2.14336791376\n",
      "2.14337885645\n",
      "2.14334244858\n",
      "2.14340431813\n",
      "2.14335182295\n",
      "2.14339494794\n",
      "2.14335783502\n",
      "2.14338893576\n",
      "2.14336209939\n",
      "2.14338467147\n",
      "2.14336337731\n",
      "2.14338339356\n",
      "2.14336033825\n",
      "2.14338643253\n",
      "2.14336257005\n",
      "2.14338419665\n",
      "2.14336183296\n",
      "2.14338493793\n",
      "2.14336571757\n",
      "2.1433810532\n",
      "2.14337117602\n",
      "2.14337559484\n",
      "2.14337385455\n",
      "2.14337291632\n",
      "2.1433714273\n",
      "2.14337534348\n",
      "2.14336275632\n",
      "2.14338401039\n",
      "2.14336186569\n",
      "2.1433849052\n",
      "2.14336442798\n",
      "2.1433823428\n",
      "2.1433697248\n",
      "2.14337704606\n",
      "2.14337367974\n",
      "2.14337309112\n",
      "2.14337286202\n",
      "2.14337390876\n",
      "[[ 0.01231622  0.01231622]\n",
      " [ 0.05473167  0.05473167]\n",
      " [ 0.00872866  0.00872866]\n",
      " [-0.04529945 -0.04529945]\n",
      " [ 0.00392471  0.00392471]\n",
      " [-0.01657483 -0.01657483]\n",
      " [ 0.03964147  0.03964147]\n",
      " [ 0.05941158  0.05941158]\n",
      " [-0.00808459 -0.00808459]\n",
      " [-0.03260995 -0.03260995]\n",
      " [-0.0600212  -0.0600212 ]\n",
      " [-0.03224923 -0.03224923]\n",
      " [-0.01266691 -0.01266691]\n",
      " [ 0.05928031  0.05928031]\n",
      " [ 0.03877176  0.03877176]\n",
      " [-0.01738336 -0.01738336]\n",
      " [-0.00559343 -0.00559343]\n",
      " [-0.04525927 -0.04525927]\n",
      " [ 0.008749    0.008749  ]\n",
      " [ 0.05471348  0.05471348]\n",
      " [ 0.30934772  0.30934772]\n",
      " [ 0.21562498  0.21562498]\n",
      " [ 0.15550372  0.15550372]\n",
      " [ 0.11286043  0.11286043]\n",
      " [ 0.10008125  0.10008125]\n",
      " [ 0.13047143  0.13047143]\n",
      " [ 0.108133    0.108133  ]\n",
      " [ 0.11552487  0.11552487]\n",
      " [ 0.07667816  0.07667816]\n",
      " [ 0.02209407  0.02209407]\n",
      " [-0.00469114 -0.00469114]\n",
      " [ 0.01958089  0.01958089]\n",
      " [ 0.10627037  0.10627037]\n",
      " [ 0.11519755  0.11519755]\n",
      " [ 0.08957408  0.08957408]\n",
      " [ 0.03660632  0.03660632]\n",
      " [-0.00294313 -0.00294313]\n",
      " [ 0.00523372  0.00523372]]\n",
      "As duas colunas acima deve ser bem semelhantes.\n",
      "(Esquerda - Gradiente numerico, Direita - Seu gradiente)\n",
      "\n",
      "Se sua implementacao de backpropagation esta correta, \n",
      "a diferenca relativa devera ser pequena (menor que 1e-9). \n",
      "\n",
      "Diferenca relativa: 2.09161e-11\n",
      "\n",
      "\n",
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "9.81592840004\n",
      "Custo com os parametros (carregados do arquivo): 9.815928\n",
      "\n",
      "(este valor deve ser proximo de 0.576051 (para lambda = 3))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    # (4): Implemente a regularização na função de custo e gradiente.\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i] :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "\n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    reg = sum(np.sum(np.power(Theta1[:,1:],2),axis=1)) + sum(np.sum(np.power(Theta2[:,1:],2),axis=1))\n",
    "    reg = (vLambda/(2 * m)) * (reg)      \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) + reg  \n",
    "    \n",
    "    g3 = (a3 - y_rotulo)\n",
    "        \n",
    "    g2 = ((g3).dot(Theta2[:,1:]) * (sigmoidGradient(z2)))\n",
    "\n",
    "    a2i = np.insert(a2,0,1, axis = 1)\n",
    "        \n",
    "    #print(g3.shape)\n",
    "    #print(a2i.T.shape)\n",
    "    #print(Theta2_grad.shape)\n",
    "    Theta2_grad = Theta2_grad + (g3.T.dot(a2i))   \n",
    "        \n",
    "    xi = np.insert(X,0,1,axis = 1)\n",
    "    Theta1_grad = Theta1_grad + (g2.T.dot(xi))  \n",
    "\n",
    "    \n",
    "    Theta2_grad = (Theta2_grad/m) \n",
    "    Theta2_grad[:,1:] = Theta2_grad[:,1:] + ((vLambda/m)*Theta2[:,1:])\n",
    "    Theta1_grad = (Theta1_grad/m)\n",
    "    \n",
    "    Theta1_grad[:,1:] = Theta1_grad[:,1:] + ((vLambda/m)*Theta1[:,1:])\n",
    "    #print('end : ')\n",
    "    #print(np.ravel(Theta1_grad))\n",
    "\n",
    "    print(J)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 3;\n",
    "\n",
    "\n",
    "# executa o arquivo que contem a funca que faz a checagem do gradiente. \n",
    "# Desa vez o valor de lambda tambem e informado\n",
    "%run utils.py\n",
    "verificaGradiente(funcaoCusto_backp_reg, vLambda=vLambda)\n",
    "\n",
    "\n",
    "print('\\n\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "J, grad = funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.576051 (para lambda = 3))\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Treinando a rede neural\n",
    "\n",
    "Neste ponto, todo o código necessário para treinar a rede está pronto.\n",
    "Aqui, será utilizada a funcao `minimize` do ScyPy para treinar as funções de custo\n",
    "de forma eficiente utilizando os gradientes calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando a rede neural.......\n",
      ".......(Aguarde, pois esse processo por ser um pouco demorado.)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7729c2137236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Minimiza a funcao de custo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda),  \n\u001b[0;32m---> 16\u001b[0;31m                 method='TNC', jac=True, options={'maxiter': MaxIter})\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Coleta os pesos retornados pela função de minimização\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[0;32m--> 453\u001b[0;31m                              **options)\n\u001b[0m\u001b[1;32m    454\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'cobyla'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/tnc.py\u001b[0m in \u001b[0;36m_minimize_tnc\u001b[0;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    407\u001b[0m                                         \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxCGit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                         \u001b[0meta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstepmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                                         xtol, pgtol, rescale, callback)\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mfunv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjacv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/tnc.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-6e7aa17a7ee5>\u001b[0m in \u001b[0;36mfuncaoCusto_backp_reg\u001b[0;34m(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#print(X[i].reshape(len(X[i]),1).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mTheta1_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTheta1_grad\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mg2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;31m#print('-----------------------------')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m#print(Theta1_grad/m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.optimize\n",
    "\n",
    "print('\\nTreinando a rede neural.......')\n",
    "print('.......(Aguarde, pois esse processo por ser um pouco demorado.)\\n')\n",
    "\n",
    "# Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "# um valor maior e verifique como isso afeta o treinamento.\n",
    "MaxIter = 500\n",
    "\n",
    "# Voce tambem pode testar valores diferentes para lambda.\n",
    "vLambda = 1\n",
    "\n",
    "# Minimiza a funcao de custo\n",
    "result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "# Coleta os pesos retornados pela função de minimização\n",
    "nn_params = result.x\n",
    "\n",
    "# Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 9: Visualizando os pesos\n",
    "\n",
    "\n",
    "Uma das formas de entender o que a rede neural está aprendendo, é visualizar a representação capturada nos neurônios da camada oculta. Informalmente, dado um neurônio de uma camada oculta qualquer, uma das formas de visualizar o que esse neurônio calcula, é encontrar uma entrada *x* que o fará ser ativado (ou seja, um resultado próximo a 1). Para a rede neural que foi treinada, perceba que a $i$-ésima linha de $\\Theta^{(1)}$ é um vetor com 401 dimensões, o qual representa os parâmetros para o $i$-ésimo neurônio. Se descartarmos o termo *bias*, teremos um vetor de 400 dimensões que representa o peso para cada pixel a partir da camada de entrada. Deste modo, uma das formas de visualizar a representação capturada pelo neurônio da camada oculta, é reorganizar essas 400 dimensões em uma imagem de 20 x 20 pixels e exibi-la. \n",
    "\n",
    "O script a seguir irá exibir uma imagem com 25 unidades, cada uma correspondendo a um neurônio da camada oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizando a rede neural... \n",
      "\n",
      "(5000, 400)\n",
      "(25, 400)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f57facbf668>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\nVisualizando a rede neural... \\n')\n",
    "\n",
    "print(X.shape)\n",
    "print(Theta1[:, 1:].shape)\n",
    "\n",
    "visualizaDados(Theta1[:, 1:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 10: Predição\n",
    "\n",
    "Após treinar a rede neural, ela será utilizada para predizer\n",
    "os rótulos das amostras. Neste ponto, foi implementada a função de predição\n",
    "para que a rede neural seja capaz de prever os rótulos no conjunto de dados\n",
    "e calcular a acurácia do método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao(Theta1, Theta2, X):\n",
    "    '''\n",
    "    Prediz o rotulo de uma amostra apresentada a rede neural\n",
    "    \n",
    "    Prediz o rotulo de X ao utilizar\n",
    "    os pesos treinados na rede neural (Theta1, Theta2)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # número de amostras\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    h1 = sigmoid( np.dot(a1,Theta1.T) )\n",
    "\n",
    "    a2 = np.hstack( [np.ones([m,1]),h1] ) \n",
    "    h2 = sigmoid( np.dot(a2,Theta2.T) )\n",
    "    \n",
    "    p = np.argmax(h2,axis=1)\n",
    "    p = p+1\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "pred = predicao(Theta1, Theta2, X)\n",
    "\n",
    "print('\\nAcuracia no conjunto de treinamento: %f\\n'%( np.mean( pred == Y ) * 100) )\n",
    "\n",
    "print('\\nAcuracia esperada: 99.56% (aproximadamente)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
