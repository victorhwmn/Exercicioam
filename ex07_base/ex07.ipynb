{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"figs/LogoUFSCar.jpg\" alt=\"Logo UFScar\" width=\"110\" align=\"left\"/>  <br/> <center>Universidade Federal de São Carlos (UFSCar)<br/><font size=\"4\"> Departamento de Computação, campus Sorocaba</center></font>\n",
    "</p>\n",
    "\n",
    "<br/>\n",
    "<font size=\"4\"><center><b>Disciplina: Aprendizado de Máquina</b></center></font>\n",
    "  \n",
    "<font size=\"3\"><center>Prof. Dr. Tiago A. Almeida</center></font>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "<center><i><b>\n",
    "Atenção: não são autorizadas cópias, divulgações ou qualquer tipo de uso deste material sem o consentimento prévio dos autores.\n",
    "</center></i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Exercício - Redes Neurais Artificiais </center>\n",
    "\n",
    "Neste exercício, você irá implementar uma rede neural artificial com *backpropagation* que será aplicada na tarefa de reconhecimento de dígitos manuscritos. Antes de iniciar, é fortemente recomendado que você revise o material apresentado em aula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O problema\n",
    "\n",
    "Você foi contratado por uma grande empresa para fazer a identificação correta e automática de quais dígitos estão presentes em um conjunto de imagens. Essas imagens têm dimensão de 20 x 20 pixels, onde cada pixel é representado por um ponto flutuante que indica a intensidade de tons de cinza naquela região.\n",
    "\n",
    "Sabe-se que a aplicação de redes neurais utilizando o algoritmo de *backpropagation* neste tipo de problema obtêm resultados satisfatórios. Assim, seu desafio é implementar tal algoritmo e encontrar os pesos ótimos para que a rede seja capaz de identificar automaticamente os dígitos contidos nas imagens.\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 0px; float: center;\">\n",
    "    <img src=\"figs/digitos.png\"  style=\"height:400px;\"/> \n",
    "    <center><em>Figura 1. Amostras do conjunto de dados.</em></center>\n",
    "</div>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Carregando e visualizando os dados\n",
    "\n",
    "Nessa etapa, você irá completar a função para plotar os dados. O conjunto que você utilizará será de digitos manuscritos (Figura 1).\n",
    "\n",
    "Cada imagem tem dimensão de 20 x 20 pixels e cada pixel é representado por um ponto flutuante com a intensidade do tom de cinza naquela região. Deste modo, cada amostra é representada pelo desdobramento dos pixels em um vetor com 400 dimensões.\n",
    "\n",
    "O conjunto de dados contém 5.000 amostras, sendo cada amostra representada por um vetor com 400 dimensões. Portanto, o conjunto é representado por uma matriz [5000,400].\n",
    "\n",
    "A segunda parte do conjunto de dados é um vetor $y$ com 5.000 dimensões, o qual contém os rótulos para cada amostra da base de treino. Imagens contendo dígitos de 1 a 9 recebem, respectivamente, classes de 1 a 9, enquanto imagens contendo o dígito 0 são rotuladas como 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro, vamos carregar os dados do arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np  # importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd # importa a biblioteca usada para trabalhar com dataframes (dados em formato de tabela) e análise de dados\n",
    "\n",
    "# importa o arquivo e guarda em um dataframe do Pandas\n",
    "df_dataset = pd.read_csv( 'dados.csv', sep=',', header=None) \n",
    "\n",
    "print('Dados carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos guardar os valores dentro de um array X e as classes dentro de um vetor Y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X:'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y: [10 10 10 10 10]\n",
      "\n",
      "Dimensao de X:  (5000, 400)\n",
      "\n",
      "Dimensao de Y:  (5000,)\n",
      "\n",
      "Classes do problema:  [ 1  2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "# Pega os valores das n-1 primeiras colunas e guarda em uma matrix X\n",
    "X = df_dataset.iloc[:, 0:-1].values \n",
    "\n",
    "# Pega os valores da ultima coluna e guarda em um vetor Y\n",
    "Y = df_dataset.iloc[:, -1].values \n",
    "\n",
    "# Imprime as 5 primeiras linhas da matriz X\n",
    "display('X:', X[0:5,:])\n",
    "\n",
    "# Imprime os 5 primeiros valores de Y\n",
    "print('Y:', Y[0:5])\n",
    "\n",
    "print('\\nDimensao de X: ', X.shape)\n",
    "\n",
    "print('\\nDimensao de Y: ', Y.shape)\n",
    "\n",
    "print('\\nClasses do problema: ', np.unique(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos plotar aleatoriamente 100 amostras da base de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2278265f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizaDados(X):\n",
    "    example_width = int(round(np.sqrt(X.shape[1])) )\n",
    "\n",
    "    # Calcula numero de linhas e colunas\n",
    "    m, n = X.shape\n",
    "    example_height = int(n / example_width)\n",
    "\n",
    "    # Calcula numero de itens que serao exibidos\n",
    "    display_rows = int(np.floor(np.sqrt(m)))\n",
    "    display_cols = int(np.ceil(m / display_rows))\n",
    "\n",
    "    fig, axs = plt.subplots(display_rows,display_cols, figsize=(7, 7))\n",
    "                        \n",
    "    for ax, i in zip(axs.ravel(), range( X.shape[0] )):\n",
    "    \n",
    "        new_X = np.reshape( np.ravel(X[i,:]), (example_width, example_height) )\n",
    "\n",
    "        ax.imshow(new_X.T, cmap='gray'); \n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "idx_perm = np.random.permutation( range(X.shape[0]) )\n",
    "visualizaDados( X[idx_perm[0:100],:] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Carregando os parâmetros\n",
    "\n",
    "A rede neural proposta para este exercício tem 3 camadas: uma camada de entrada, uma camada oculta e uma camada de saída  (Figura 2). É importante lembrar que a camada de entrada possui 400 neurônios devido ao desdobramento dos pixels das amostras em vetores com 400 dimensões (sem considerar o *bias*, sempre +1).\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 5px; float: center;\">\n",
    "    <img src=\"figs/nn.png\"  style=\"height:350px;\"/> \n",
    "    <center><em>Figura 2. Arquitetura da rede neural.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Inicialmente, você terá acesso aos parâmetros ($\\Theta^{(1)}$, $\\Theta^{(2)}$) de uma rede neural já treinada, armazenados nos arquivos **pesos_Theta1.csv** e **pesos_Theta2.csv**. Tais parâmetros têm dimensões condizentes com uma rede neural com 25 neurônios na camada intermediária e 10 neurônios na camada de saída (correspondente às dez possíveis classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, vamos carregar os pesos pré-treinados para a rede neural e inicializar os parâmetros mais importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando parametros salvos da rede neural...\n",
      "\n",
      "Pesos carregados com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# parametros a serem utilizados neste exercicio\n",
    "input_layer_size  = 400  # 20x20 dimensao das imagens de entrada\n",
    "hidden_layer_size = 25   # 25 neuronios na camada oculta\n",
    "num_labels = 10          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "    \n",
    "print('\\nCarregando parametros salvos da rede neural...\\n')\n",
    "\n",
    "# carregando os pesos da camada 1\n",
    "Theta1 = pd.read_csv('pesos_Theta1.csv', sep=',', header=None).values\n",
    "\n",
    "# carregando os pesos da camada 2\n",
    "Theta2 = pd.read_csv('pesos_Theta2.csv', sep=',', header=None).values\n",
    "\n",
    "# concatena os pesos em um único vetor\n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "\n",
    "print('Pesos carregados com sucesso!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Calcula o custo (*Feedforward*)\n",
    "\n",
    "Você precisará implementar a função de custo e gradiente para a rede neural. A função de custo (sem regularização) é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "Na função $J$, $h_\\Theta(x^{(i)})$ é computado conforme representado na Figura 2. A constante $K$ representa o número de classes. Assim, $h_\\Theta(x^{(i)})_k$ = $a^{(3)}_k$ corresponde à ativação (valor de saída) da $k$-ésima unidade de saída. Também, é importante lembrar que o vetor de saída precisa ser criado a partir da classe original, tornando-se compatível com a rede neural, ou seja, espera-se vetores com 10 posições contendo 1 para o elemento referente à classe esperada e 0 nos demais elementos. Por exemplo, seja 5 o rótulo de determinada amostra, o vetor $Y$ correspondente terá 1 na posição $y_5$ e 0 nas demais posições.\n",
    "\n",
    "Você precisará implementar o algoritmo *feedfoward* para calcular $h_\\Theta(x^{(i)})$ para cada amostra $i$ e somar o custo de todas as amostras. Seu código deverá ser flexível para funcionar com conjuntos de dados de qualquer tamanho e qualquer quantidade de classes, considerando $K \\geq 3$.\n",
    "\n",
    "Nesta fase, implemente a função de custo sem regularização para facilitar a análise. Posteriormente, você implementará o custo regularizado.\n",
    "\n",
    "Antes de implementar a função de custo, você precisará da função sigmoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid de 0 = 0.500000\n",
      "sigmoid de 10 = 0.999955\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula a função sigmoidal  \n",
    "    \"\"\"\n",
    "\n",
    "    z = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return z\n",
    "\n",
    "# testando a função sigmoidal\n",
    "z = sigmoid(0)\n",
    "print('sigmoid de 0 = %1.6f' %(z))\n",
    "\n",
    "z = sigmoid(10)\n",
    "print('sigmoid de 10 = %1.6f' %(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete a função que será usada para calcular o custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Funcao de custo sem regularizacao ...\n",
      "\n",
      "[[  1.12661530e-04   1.74127856e-03   2.52696959e-03 ...,   4.01468105e-04\n",
      "    6.48072305e-03   9.95734012e-01]\n",
      " [  4.79026796e-04   2.41495958e-03   3.44755685e-03 ...,   2.39107046e-03\n",
      "    1.97025086e-03   9.95696931e-01]\n",
      " [  8.85702310e-05   3.24266731e-03   2.55419797e-02 ...,   6.22892325e-02\n",
      "    5.49803551e-03   9.28008397e-01]\n",
      " ..., \n",
      " [  5.17641791e-02   3.81715020e-03   2.96297510e-02 ...,   2.15667361e-03\n",
      "    6.49826950e-01   2.42384687e-05]\n",
      " [  8.30631310e-04   6.22003774e-04   3.14518512e-04 ...,   1.19366192e-02\n",
      "    9.71410499e-01   2.06173648e-04]\n",
      " [  4.81465717e-05   4.58821829e-04   2.15146201e-05 ...,   5.73434571e-03\n",
      "    6.96288990e-01   8.18576980e-02]]\n",
      "Custo com os parametros (carregados do arquivo): 0.287629 \n",
      "\n",
      "(este valor deve ser proximo de 0.287629)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    #display(y[0:30])\n",
    "    \n",
    "    #a1 = (((-y) * np.log(z1).T) - ((1-y) * (np.log(1-z1).T) ))\n",
    "    #a1 = a1.T\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    #a3 = a3.T\n",
    "    #print(sum(sum(a2))/m)\n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) \n",
    "    \n",
    "\n",
    "    print(a3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "print('\\nFuncao de custo sem regularizacao ...\\n')\n",
    "\n",
    "J = funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f ' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.287629)\\n')\n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 4: Regularização\n",
    "\n",
    "A função de custo com regularização é descrita a seguir.\n",
    "\n",
    "$$J(\\Theta) = \\frac{1}{m} \\sum_{i=1}^{m}\\sum_{k=1}^{K} \\left[-y_k^{(i)} \\log\\left( \\left(h_\\Theta(x^{(i)})\\right)_k \\right) - \\left(1 - y_k^{(i)}\\right) \\log \\left( 1 - \\left(h_\\Theta(x^{(i)} )\\right)_k \\right)\\right]$$\n",
    "\n",
    "$$ + \\frac{\\lambda}{2m} \\left[\\sum_{j=1}^{25} \\sum_{k=1}^{400} \\left(\\Theta^{(1)}_{j,k}\\right)^2 + \\sum_{j=1}^{10} \\sum_{k=1}^{25} \\left(\\Theta^{(2)}_{j,k}\\right)^2\\right]$$\n",
    "\n",
    "Você pode assumir que a rede neural terá 3 camadas - uma camada de entrada, uma camada oculta e uma camada de saída. No entanto, seu código deverá ser flexível para suportar qualquer quantidade de neurônios em cada uma dessas camadas. Embora a função $J$ descrita anteriormente contenha números fixos para $\\Theta^{(1)}$ e $\\Theta^{(2)}$, seu código deverá funcionar para outros tamanhos.\n",
    "\n",
    "Também, é importante que a regularização não seja aplicada a termos relacionados aos *bias*. Neste contexto, estes termos estão na primeira coluna de cada matriz $\\Theta^{(1)}$ e $\\Theta^{(2)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, complete a nova função de custo aplicando regularização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    #\n",
    "    # (3): Implemente a regularização na função de custo.\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    reg = sum(np.sum(np.power(Theta1[:,1:],2),axis=1)) + sum(np.sum(np.power(Theta2[:,1:],2),axis=1))\n",
    "    reg = (vLambda/(2 * m)) * (reg)      \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1)) + reg\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.383770 \n",
      "\n",
      "(este valor deve ser proximo de 0.383770)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "# Parametro de regularizacao dos pesos (aqui sera igual a 1).\n",
    "vLambda = 1;\n",
    "\n",
    "J = funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f ' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.383770)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 5: Inicialização dos parâmetros\n",
    "\n",
    "Nesta parte, começa a implementação das duas camadas da rede neural para classificação dos dígitos manuscritos. Os pesos são inicializados aleatoriamente. Mas, para que toda a execução gere o mesmo resultado, vamos usar uma semente para a função de geração de números aleatórios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicializando parametros da rede neural...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inicializaPesosAleatorios(L_in, L_out, randomSeed = None):\n",
    "    '''\n",
    "    Inicializa aleatoriamente os pesos de uma camada usando \n",
    "    L_in (conexoes de entrada) e L_out (conexoes de saida).\n",
    "\n",
    "    W sera definido como uma matriz de dimensoes [L_out, 1 + L_in]\n",
    "    visto que devera armazenar os termos para \"bias\".\n",
    "    \n",
    "    randomSeed: indica a semente para o gerador aleatorio\n",
    "    '''\n",
    "\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # se for fornecida uma semente para o gerador aleatorio\n",
    "    if randomSeed is not None:\n",
    "        W = np.random.RandomState(randomSeed).rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    # se nao for fornecida uma semente para o gerador aleatorio\n",
    "    else:\n",
    "        W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "print('\\nInicializando parametros da rede neural...\\n')\n",
    "    \n",
    "initial_Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "initial_Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)\n",
    "\n",
    "# junta os pesos iniciais em um unico vetor\n",
    "initial_rna_params = np.concatenate([np.ravel(initial_Theta1), np.ravel(initial_Theta2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 6: Backpropagation\n",
    "\n",
    "Nesta parte do exercício, você implementará o algoritmo de *backpropagation* responsável por calcular o gradiente para a função de custo da rede neural. Terminada a implementação do cálculo do gradiente, você poderá treinar a rede neural minimizando a função de custo $J(\\Theta)$ usando um otimizador avançado, como a funcao `minimize` do ScyPy.\n",
    "\n",
    "Primeiro, você precisará implementar o gradiente para a rede neural sem regularização. Após ter verificado que o cálculo do gradiente está correto, você implementará o gradiente para a rede neural com regularização.\n",
    "\n",
    "Você deverá começar pela implementação do gradiente da sigmóide, o qual pode ser calculado utilizando a equação:\n",
    "\n",
    "$$ g'(z) = \\frac{d}{dz}g(z) = g(z)(1-g(z)), $$\n",
    "\n",
    "sendo que\n",
    "\n",
    "$$ g(z) = \\frac{1}{1 + e^{-z}}. $$\n",
    "\n",
    "Ao completar, teste diferentes valores para a função `sigmoidGradient`. Para valores grandes de *z* (tanto positivo, quanto negativo), o resultado deve ser próximo a zero. Quando $z = 0$, o resultado deve ser exatamente 0,25. A função deve funcionar com vetores e matrizes também. No caso de matrizes, a função deve calcular o gradiente para cada elemento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando o gradiente da sigmoide...\n",
      "\n",
      "Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\n",
      "\n",
      "[ 0.19661193  0.23500371  0.25        0.23500371  0.19661193]\n",
      "\n",
      "Se sua implementacao estiver correta, o gradiente da sigmoide sera:\n",
      "[0.19661193 0.23500371 0.25 0.23500371 0.19661193]:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    '''\n",
    "    Retorna o gradiente da funcao sigmoidal para z \n",
    "    \n",
    "    Calcula o gradiente da funcao sigmoidal\n",
    "    para z. A funcao deve funcionar independente se z for matriz ou vetor.\n",
    "    Nestes casos,  o gradiente deve ser calculado para cada elemento.\n",
    "    '''\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Calcula o gradiente da funcao sigmoidal para \n",
    "    #           cada valor de z (seja z matriz, escalar ou vetor).\n",
    "    #\n",
    "\n",
    "    g = (1/(1+np.exp(-z))) * (1- (1/(1+np.exp(-z))))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ##########################################################################\n",
    "\n",
    "    return g\n",
    "\n",
    "print('\\nAvaliando o gradiente da sigmoide...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([1,-0.5, 0, 0.5, 1]))\n",
    "print('Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\\n')\n",
    "print(g)\n",
    "\n",
    "print('\\nSe sua implementacao estiver correta, o gradiente da sigmoide sera:')\n",
    "print('[0.19661193 0.23500371 0.25 0.23500371 0.19661193]:\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O objetivo do algoritmo *backpropagation* é encontrar a \"parcela de responsabilidade\" que cada neurônio da rede neural teve com o erro gerado na saída. Dada uma amostra de treino ($x^{(t)}, y^{(t)}$), primeiro é executado o passo de *feedforward* para calcular todas as ativações na rede, incluindo o valor de saída $h_{\\Theta}(x)$. Posteriormente, para cada neurônio $j$ na camada $l$, é calculado o \"erro\" $\\delta_{j}^{(l)}$ que mede o quanto determinado neurônio contribuiu para a diferença entre o valor esperado e o valor obtido na saída da rede.\n",
    "\n",
    "<center>\n",
    "<div style=\"padding: 5px; float: center;\">\n",
    "    <img src=\"figs/nn_back.png\"  style=\"height:350px;\"/> \n",
    "    <center><em>Figura 3. Arquitetura da rede neural.</em></center>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "Nos neurônios de saída, a diferença pode ser medida entre o valor esperado (rótulo da amostra) e o valor obtido (a ativação final da rede), onde tal diferença será usada para definir $\\delta_{j}^{(3)}$ (visto que a camada 3 é a última). Nas camadas ocultas (quando houver mais de uma), o termo $\\delta_{j}^{(l)}$ será calculado com base na média ponderada dos erros encontrados na camada posterior ($l + 1$).\n",
    "\n",
    "A seguir, é descrito em detalhes como a implementação do algoritmo *backpropagation* deve ser feita. Você precisará seguir os passos 1 a 4 dentro de um laço, processando uma amostra por vez. No passo 5, o gradiente acumulado é dividido pelas *m* amostras, o qual será utilizado na função de custo da rede neural.\n",
    "\n",
    " 1. Coloque os valores na camada de entrada ($a^{(1)}$) para a amostra de treino a ser processada. Calcule as ativações das camadas 2 e 3 utilizando o passo de *feedforward*. Observe que será necessário adicionar um termo $+1$ para garantir que os vetores de ativação das camadas ($a^{(1)}$) e ($a^{(2)}$) também incluam o neurônio de *bias*.\n",
    " \n",
    " 2. Para cada neurônio $k$ na camada 3 (camada de saída), defina:\n",
    "    $$ \\delta_{k}^{(3)} = (a^{(3)}_k - y_k), $$\n",
    "    onde $y_k \\in \\{0,1\\}$ indica se a amostra sendo processada pertence a classe $k$ ($y_k = 1$) ou não ($y_k = 0$).\n",
    "    \n",
    " 3. Para a camada oculta $l$ = 2, defina:\n",
    "\n",
    "    $$ \\delta^{(2)} = (\\Theta^{(2)})^T \\delta^{(3)}*g'(z^{(2)}) $$\n",
    "    \n",
    " 4. Acumule o gradiente usando a fórmula descrita a seguir. Lembre-se de não utilizar o valor associado ao bias $\\delta^{(2)}_0$.\n",
    "    \n",
    "    $$ \\Delta^{(l)} = \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T $$\n",
    "    \n",
    " 5. Obtenha o gradiente sem regularização para a função de custo da rede neural dividindo os gradientes acumulados por $\\frac{1}{m}$:\n",
    " \n",
    "     $$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} $$\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "Neste ponto, você deverá implementar o algoritmo de *backpropagation*. Crie uma nova função de custo, baseada na função implementada anteriormente, que retorne as derivadas parciais dos parâmetros. Nesta função, você precisará implementar o gradiente para a rede neural sem regularização.\n",
    "\n",
    "Logo após a função que implementa o algoritmo de *backpropagation*, é chamada uma outra função que fará a checagem do gradiente. O código dessa função está no arquivo **utils.py** que está localizado na pasta raíz desse exercício. Esta checagem tem o propósito de certificar que seu código calcula o gradiente corretamente. Neste passo, se sua implementação estiver correta, você deverá ver uma diferença **menor que 1e-9**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "(5, 4)\n",
      "(3, 6)\n",
      "[[  1.76654043e-02   1.76654044e-02]\n",
      " [  4.83605807e-04   4.83605809e-04]\n",
      " [  7.06767167e-04   7.06767168e-04]\n",
      " [  2.80130052e-04   2.80130052e-04]\n",
      " [  9.70879575e-03   9.70879576e-03]\n",
      " [  2.36708468e-04   2.36708468e-04]\n",
      " [  2.83653863e-04   2.83653862e-04]\n",
      " [  6.98092006e-05   6.98092035e-05]\n",
      " [ -7.18128122e-03  -7.18128123e-03]\n",
      " [ -2.28610917e-04  -2.28610917e-04]\n",
      " [ -4.01542821e-04  -4.01542820e-04]\n",
      " [ -2.05298107e-04  -2.05298106e-04]\n",
      " [ -1.74827533e-02  -1.74827533e-02]\n",
      " [ -4.83497640e-04  -4.83497643e-04]\n",
      " [ -7.17048672e-04  -7.17048672e-04]\n",
      " [ -2.91348459e-04  -2.91348459e-04]\n",
      " [ -1.16920643e-02  -1.16920643e-02]\n",
      " [ -2.93754241e-04  -2.93754241e-04]\n",
      " [ -3.73295379e-04  -3.73295381e-04]\n",
      " [ -1.09630474e-04  -1.09630470e-04]\n",
      " [  1.09347722e-01   1.09347722e-01]\n",
      " [  5.67965185e-02   5.67965185e-02]\n",
      " [  5.25298306e-02   5.25298306e-02]\n",
      " [  5.53542907e-02   5.53542907e-02]\n",
      " [  5.59290833e-02   5.59290833e-02]\n",
      " [  5.23534682e-02   5.23534682e-02]\n",
      " [  1.08133003e-01   1.08133003e-01]\n",
      " [  5.67319602e-02   5.67319602e-02]\n",
      " [  5.14442931e-02   5.14442931e-02]\n",
      " [  5.48296085e-02   5.48296085e-02]\n",
      " [  5.56926532e-02   5.56926532e-02]\n",
      " [  5.11795651e-02   5.11795651e-02]\n",
      " [  5.06270372e-01   5.06270372e-01]\n",
      " [  2.63880175e-01   2.63880175e-01]\n",
      " [  2.41215476e-01   2.41215476e-01]\n",
      " [  2.57977109e-01   2.57977109e-01]\n",
      " [  2.58731922e-01   2.58731922e-01]\n",
      " [  2.40983787e-01   2.40983787e-01]]\n",
      "As duas colunas acima deve ser bem semelhantes.\n",
      "(Esquerda - Gradiente numerico, Direita - Seu gradiente)\n",
      "\n",
      "Se sua implementacao de backpropagation esta correta, \n",
      "a diferenca relativa devera ser pequena (menor que 1e-9). \n",
      "\n",
      "Diferenca relativa: 1.91433e-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para a tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    #print(y)\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "\n",
    "    g2 = np.zeros(Theta2[:,1:].shape)\n",
    "    \n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    \n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    J = (1/m) * sum(np.sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )),axis=1))\n",
    "    \n",
    "    g3 = (a3 - y_rotulo)\n",
    "    #print(a3)    \n",
    "    g2 = (np.dot(g3,Theta2[:,1:]) * (sigmoidGradient(z2)))\n",
    "    #print(np.dot(g3,Theta2[:,1:]).shape)\n",
    "    #print(z2.shape)\n",
    "\n",
    "    a2i = np.insert(a2,0,1, axis = 1)\n",
    "        \n",
    "    #print(g2.shape)\n",
    "    #print(a2i.T.shape)\n",
    "    #print(Theta2_grad.shape)\n",
    "    Theta2_grad = Theta2_grad + (g3.T.dot(a2i))   \n",
    "        \n",
    "        #print(X[i].reshape(len(X[i]),1).shape)\n",
    "    xi = np.insert(X,0,1,axis = 1)\n",
    "    Theta1_grad = Theta1_grad + (g2.T.dot(xi))  \n",
    "        #print('-----------------------------')\n",
    "    Theta2_grad = Theta2_grad/m\n",
    "    Theta1_grad = Theta1_grad/m\n",
    "    print(Theta1_grad.shape)\n",
    "    print(Theta2_grad.shape)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# executa o arquivo que contem a funca que faz a checagem do gradiente\n",
    "%run utils.py\n",
    "verificaGradiente(funcaoCusto_backp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 7: Outra parte da regularização\n",
    "\n",
    "Agora, a regularização deverá ser adicionada após se calcular o gradiente durante o algoritmo de *backpropagation*. Lembre-se que a regularização não é adicionada quando $j = 0$, ou seja, na primeira coluna de $\\Theta$. Portanto, para $j \\geq 1$, o gradiente é descrito como:\n",
    "\n",
    "$$ D^{(l)}_{ij} = \\frac{1}{m}\\Delta^{(l)}_{ij} + \\frac{\\lambda}{m}\\Theta^{(l)}_{ij} $$\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "Você deverá criar uma nova função de custo que é uma atualização da função anterior, mas com regularização e gradiente.\n",
    "\n",
    "Logo após a função que implementa o algoritmo de *backpropagation* com regularização, a função que faz a checagem do gradiente será chamada novamente. Se sua implementação estiver correta, você deverá ver uma diferença **menor que 1e-9**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0176654   0.0176654 ]\n",
      " [ 0.05504145  0.05504145]\n",
      " [ 0.00917397  0.00917397]\n",
      " [-0.04512802 -0.04512802]\n",
      " [ 0.0097088   0.0097088 ]\n",
      " [-0.01652822 -0.01652822]\n",
      " [ 0.03970285  0.03970285]\n",
      " [ 0.0594313   0.0594313 ]\n",
      " [-0.00718128 -0.00718128]\n",
      " [-0.03286988 -0.03286988]\n",
      " [-0.06040096 -0.06040096]\n",
      " [-0.03239967 -0.03239967]\n",
      " [-0.01748275 -0.01748275]\n",
      " [ 0.05895294  0.05895294]\n",
      " [ 0.03830022  0.03830022]\n",
      " [-0.01756555 -0.01756555]\n",
      " [-0.01169206 -0.01169206]\n",
      " [-0.04535299 -0.04535299]\n",
      " [ 0.00861934  0.00861934]\n",
      " [ 0.05466708  0.05466708]\n",
      " [ 0.10934772  0.10934772]\n",
      " [ 0.11135436  0.11135436]\n",
      " [ 0.06099703  0.06099703]\n",
      " [ 0.00994614  0.00994614]\n",
      " [-0.00160637 -0.00160637]\n",
      " [ 0.03558854  0.03558854]\n",
      " [ 0.108133    0.108133  ]\n",
      " [ 0.11609346  0.11609346]\n",
      " [ 0.0761714   0.0761714 ]\n",
      " [ 0.02218834  0.02218834]\n",
      " [-0.00430676 -0.00430676]\n",
      " [ 0.01898519  0.01898519]\n",
      " [ 0.50627037  0.50627037]\n",
      " [ 0.32331662  0.32331662]\n",
      " [ 0.28023275  0.28023275]\n",
      " [ 0.24070291  0.24070291]\n",
      " [ 0.20104807  0.20104807]\n",
      " [ 0.19592455  0.19592455]]\n",
      "As duas colunas acima deve ser bem semelhantes.\n",
      "(Esquerda - Gradiente numerico, Direita - Seu gradiente)\n",
      "\n",
      "Se sua implementacao de backpropagation esta correta, \n",
      "a diferenca relativa devera ser pequena (menor que 1e-9). \n",
      "\n",
      "Diferenca relativa: 1.78982e-11\n",
      "\n",
      "\n",
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 0.576051\n",
      "\n",
      "(este valor deve ser proximo de 0.576051 (para lambda = 3))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    # (4): Implemente a regularização na função de custo e gradiente.\n",
    "    #\n",
    "\n",
    "    eps = 1e-15\n",
    "    y_rotulo = np.zeros((len(y),num_labels),dtype=int)\n",
    "    for i in range(len(y)) :\n",
    "        for j in range(num_labels) :\n",
    "            if j == y[i]-1 :\n",
    "                y_rotulo[i][j] = 1\n",
    "            else :\n",
    "                y_rotulo[i][j] = 0\n",
    "\n",
    "    z2 = np.insert(X,0,1,axis = 1).dot(Theta1.T)\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    z3 = np.insert(a2,0,1,axis = 1).dot(Theta2.T)\n",
    "    a3 = sigmoid(z3) \n",
    "    \n",
    "    reg = sum(np.sum(np.power(Theta1[:,1:],2),axis=1)) + sum(np.sum(np.power(Theta2[:,1:],2),axis=1))\n",
    "    reg = (vLambda/(2 * m)) * (reg)      \n",
    "    \n",
    "    \n",
    "    J = (1/m) * sum(sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )))) + reg  \n",
    "    #print(reg)\n",
    "    #print( (1/m) * sum(sum(((-y_rotulo) * np.log(a3 + eps)) - ((1-y_rotulo) * (np.log(1-a3 + eps) )))))\n",
    "    \n",
    "    g3 = (a3 - y_rotulo)\n",
    "        \n",
    "    g2 = ((g3).dot(Theta2[:,1:]) * (sigmoidGradient(z2)))\n",
    "\n",
    "    a2i = np.insert(a2,0,1, axis = 1)\n",
    "        \n",
    "    #print(g3.shape)\n",
    "    #print(a2i.T.shape)\n",
    "    #print(Theta2_grad.shape)\n",
    "    Theta2_grad = Theta2_grad + (g3.T.dot(a2i))   \n",
    "        \n",
    "    xi = np.insert(X,0,1,axis = 1)\n",
    "    Theta1_grad = Theta1_grad + (g2.T.dot(xi))  \n",
    "\n",
    "    \n",
    "    Theta2_grad = (Theta2_grad/m) \n",
    "    Theta2_grad[:,1:] = Theta2_grad[:,1:] + ((vLambda/m)*Theta2[:,1:])\n",
    "    Theta1_grad = (Theta1_grad/m)\n",
    "    \n",
    "    Theta1_grad[:,1:] = Theta1_grad[:,1:] + ((vLambda/m)*Theta1[:,1:])\n",
    "    #print('end : ')\n",
    "    #print(np.ravel(Theta1_grad))\n",
    "    #print(J)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 3;\n",
    "\n",
    "\n",
    "# executa o arquivo que contem a funca que faz a checagem do gradiente. \n",
    "# Desa vez o valor de lambda tambem e informado\n",
    "%run utils.py\n",
    "verificaGradiente(funcaoCusto_backp_reg, vLambda=vLambda)\n",
    "\n",
    "\n",
    "print('\\n\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "J, grad = funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f' %J)\n",
    "print('\\n(este valor deve ser proximo de 0.576051 (para lambda = 3))\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 8: Treinando a rede neural\n",
    "\n",
    "Neste ponto, todo o código necessário para treinar a rede está pronto.\n",
    "Aqui, será utilizada a funcao `minimize` do ScyPy para treinar as funções de custo\n",
    "de forma eficiente utilizando os gradientes calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando a rede neural.......\n",
      ".......(Aguarde, pois esse processo por ser um pouco demorado.)\n",
      "\n",
      "     fun: 0.3144191208402195\n",
      "     jac: array([ -4.05558152e-05,  -1.01724948e-07,   9.09124034e-09, ...,\n",
      "        -4.84460679e-06,   9.73372681e-06,   1.99005450e-05])\n",
      " message: 'Max. number of function evaluations reached'\n",
      "    nfev: 500\n",
      "     nit: 32\n",
      "  status: 3\n",
      " success: False\n",
      "       x: array([ -1.21581092e+00,  -5.08624740e-04,   4.54562017e-05, ...,\n",
      "         3.73906205e-01,   2.19130422e+00,   1.12920299e+00])\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.optimize\n",
    "\n",
    "print('\\nTreinando a rede neural.......')\n",
    "print('.......(Aguarde, pois esse processo por ser um pouco demorado.)\\n')\n",
    "\n",
    "# Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "# um valor maior e verifique como isso afeta o treinamento.\n",
    "MaxIter = 500\n",
    "\n",
    "# Voce tambem pode testar valores diferentes para lambda.\n",
    "vLambda = 1\n",
    "\n",
    "# Minimiza a funcao de custo\n",
    "result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "# Coleta os pesos retornados pela função de minimização\n",
    "nn_params = result.x\n",
    "\n",
    "# Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 9: Visualizando os pesos\n",
    "\n",
    "\n",
    "Uma das formas de entender o que a rede neural está aprendendo, é visualizar a representação capturada nos neurônios da camada oculta. Informalmente, dado um neurônio de uma camada oculta qualquer, uma das formas de visualizar o que esse neurônio calcula, é encontrar uma entrada *x* que o fará ser ativado (ou seja, um resultado próximo a 1). Para a rede neural que foi treinada, perceba que a $i$-ésima linha de $\\Theta^{(1)}$ é um vetor com 401 dimensões, o qual representa os parâmetros para o $i$-ésimo neurônio. Se descartarmos o termo *bias*, teremos um vetor de 400 dimensões que representa o peso para cada pixel a partir da camada de entrada. Deste modo, uma das formas de visualizar a representação capturada pelo neurônio da camada oculta, é reorganizar essas 400 dimensões em uma imagem de 20 x 20 pixels e exibi-la. \n",
    "\n",
    "O script a seguir irá exibir uma imagem com 25 unidades, cada uma correspondendo a um neurônio da camada oculta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizando a rede neural... \n",
      "\n",
      "(5000, 400)\n",
      "(25, 400)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f226d4edeb8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('\\nVisualizando a rede neural... \\n')\n",
    "\n",
    "print(X.shape)\n",
    "print(Theta1[:, 1:].shape)\n",
    "\n",
    "visualizaDados(Theta1[:, 1:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 10: Predição\n",
    "\n",
    "Após treinar a rede neural, ela será utilizada para predizer\n",
    "os rótulos das amostras. Neste ponto, foi implementada a função de predição\n",
    "para que a rede neural seja capaz de prever os rótulos no conjunto de dados\n",
    "e calcular a acurácia do método."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Acuracia no conjunto de treinamento: 99.580000\n",
      "\n",
      "\n",
      "Acuracia esperada: 99.56% (aproximadamente)\n"
     ]
    }
   ],
   "source": [
    "def predicao(Theta1, Theta2, X):\n",
    "    '''\n",
    "    Prediz o rotulo de uma amostra apresentada a rede neural\n",
    "    \n",
    "    Prediz o rotulo de X ao utilizar\n",
    "    os pesos treinados na rede neural (Theta1, Theta2)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # número de amostras\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    h1 = sigmoid( np.dot(a1,Theta1.T) )\n",
    "\n",
    "    a2 = np.hstack( [np.ones([m,1]),h1] ) \n",
    "    h2 = sigmoid( np.dot(a2,Theta2.T) )\n",
    "    \n",
    "    p = np.argmax(h2,axis=1)\n",
    "    p = p+1\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "pred = predicao(Theta1, Theta2, X)\n",
    "\n",
    "print('\\nAcuracia no conjunto de treinamento: %f\\n'%( np.mean( pred == Y ) * 100) )\n",
    "\n",
    "print('\\nAcuracia esperada: 99.56% (aproximadamente)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
